{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is torch.nn really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch提供了一个非常优美的模块和类，比如torch.nn,torch.optim,Dataset和DataLoader，他们帮助我们创建和训练神经网络。为了更好地使用他们的力量和针对我们的问题定制化模块，你需要精确理解他们在做什么。为了帮助我们理解，我们首先不用任何的pytorch Module，只用基本的Tensor操作训练一个基本的神经网络，用于识别MNIST数据集。然后我们逐步增加torch.nn,torch.optim,Dataset和DataLoader。通过这样的方式精确地展示每个部分是怎么工作的，以及如何让我们的代码更加简练、更加灵活。\n",
    "\n",
    "**这个教程假设你安装好了pytorch，并对相关操作有基本的了解。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要使用基本的MNIST数据集，它由黑白的手写数字组成。\n",
    "\n",
    "我们将使用pathlib解决路径问题，使用request来下载数据集。我们将仅仅在使用模块时import，所以你能精准的看到什么时候我们使用了哪个模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数据集是numpy数组的格式。他们已经使用pickle存储——这是一种python内的序列化数据的模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个图像都是28x28，它被保存为行为784的向量。因此想要展示图像，我们需要把它reshape为28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch使用torch.tensor而不是numpy array，所以我们需要转化数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " torch.Size([50000, 784]),\n",
       " tensor(0),\n",
       " tensor(9))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "n, c = x_train.shape\n",
    "x_train, x_train.shape, y_train.min(), y_train.max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net from scratch (no torch.nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先创建一个只用tensor运算的模型。\n",
    "\n",
    "PyTorch提供随机或初始为零的创建tensor方法，我们要用它为我们的神经网络来创建w和b。它们只是基本的tensor，我们还需要添加梯度属性，这会让pytorch在计算时记录这些运算，这样在反向传播时就可以自动地求导。\n",
    "\n",
    "对于w和b，我们设置requires_grad=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "w = torch.randn(784, 10) / math.sqrt(784)\n",
    "w.requires_grad_(True)\n",
    "b = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为pytorch具有自动计算梯度的能力，我们能使用任意的标准python函数作为model。因此我们写一个基本的矩阵乘法和广播加法来创建简单的线性模块。我们还需要激活函数，所以我们写一个log_softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb@w+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的@是矩阵乘法。我们会在forward函数里调用上面的函数。在这个阶段，我们的预测结果不会比随机猜好多少，因为w和b都是随机的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.2140, -2.5716, -2.1541, -2.5556, -2.9169, -2.3731, -2.0009, -1.8493,\n",
       "         -2.2918, -2.5290], grad_fn=<SelectBackward>),\n",
       " torch.Size([64, 10]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们看到的，preds不仅包括值，还包括梯度函数，我们将在反向传播时使用到这个。\n",
    "\n",
    "接下来我们实现负的log似然函数作为loss function。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用当前这个随机模型的输出来验证一下loss的功能，通过对比，我们能看到loss在变小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3848, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的函数用于计算模型的准确率。对于每次预测，我们选择值最大的那个索引作为预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下在随机模型下的准确率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们能运行训练循环了。对于每个迭代，我们要做：\n",
    "- 选择一个数据的mini-batch\n",
    "- 使用模型去预测结果\n",
    "- 计算loss\n",
    "- loss.backward()更新模型中参数的梯度，在这个案例中，是w和b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新参数的过程要在torch.no_grad()下，因为我们不希望在反向传播时保存梯度信息。\n",
    "\n",
    "然后要记得把梯度清零，这样才能开始下一轮的训练。因为loss.backward()会叠加梯度而不是替换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19133776426315308 tensor(0.9531)\n",
      "0.1927119642496109 tensor(0.9844)\n",
      "0.37512335181236267 tensor(0.9062)\n",
      "0.30613717436790466 tensor(0.8906)\n",
      "0.3707409203052521 tensor(0.8750)\n",
      "0.07918106019496918 tensor(0.9688)\n",
      "0.17526431381702423 tensor(0.9219)\n",
      "0.43739959597587585 tensor(0.9062)\n",
      "0.17044353485107422 tensor(0.9375)\n",
      "0.34060826897621155 tensor(0.9219)\n",
      "0.317685604095459 tensor(0.9531)\n",
      "0.18646810948848724 tensor(0.9375)\n",
      "0.2737189829349518 tensor(0.9062)\n",
      "0.5141398906707764 tensor(0.8594)\n",
      "0.4013713300228119 tensor(0.8906)\n",
      "0.2668895423412323 tensor(0.9219)\n",
      "0.6238927841186523 tensor(0.8438)\n",
      "0.6400451064109802 tensor(0.7812)\n",
      "0.07718885689973831 tensor(0.9688)\n",
      "0.5690637826919556 tensor(0.8750)\n",
      "0.3439263701438904 tensor(0.8906)\n",
      "0.6506285071372986 tensor(0.8594)\n",
      "0.220356747508049 tensor(0.9062)\n",
      "0.28316807746887207 tensor(0.8906)\n",
      "0.30555403232574463 tensor(0.9062)\n",
      "0.14686985313892365 tensor(0.9688)\n",
      "0.16125500202178955 tensor(0.9688)\n",
      "0.1654016375541687 tensor(0.9531)\n",
      "0.22721987962722778 tensor(0.9531)\n",
      "0.08585517853498459 tensor(0.9844)\n",
      "0.19295279681682587 tensor(0.9219)\n",
      "0.22401726245880127 tensor(0.9375)\n",
      "0.2935127317905426 tensor(0.9375)\n",
      "0.15571598708629608 tensor(0.9531)\n",
      "0.2904089689254761 tensor(0.8906)\n",
      "0.13446573913097382 tensor(0.9375)\n",
      "0.14267870783805847 tensor(0.9531)\n",
      "0.3030594289302826 tensor(0.9219)\n",
      "0.10698916018009186 tensor(0.9844)\n",
      "0.09297709167003632 tensor(0.9844)\n",
      "0.17766091227531433 tensor(0.9375)\n",
      "0.4362541437149048 tensor(0.9219)\n",
      "0.21795400977134705 tensor(0.9375)\n",
      "0.3108205497264862 tensor(0.9062)\n",
      "0.2045447826385498 tensor(0.9375)\n",
      "0.18610751628875732 tensor(0.9531)\n",
      "0.23352333903312683 tensor(0.9219)\n",
      "0.37514185905456543 tensor(0.8906)\n",
      "0.22031982243061066 tensor(0.9219)\n",
      "0.12049133330583572 tensor(0.9688)\n",
      "0.31609803438186646 tensor(0.9062)\n",
      "0.25497516989707947 tensor(0.9219)\n",
      "0.1729990541934967 tensor(0.9375)\n",
      "0.16186554729938507 tensor(0.9688)\n",
      "0.2522350549697876 tensor(0.9375)\n",
      "0.2251197099685669 tensor(0.9375)\n",
      "0.0945800393819809 tensor(0.9688)\n",
      "0.4606560468673706 tensor(0.8750)\n",
      "0.3605818450450897 tensor(0.9062)\n",
      "0.2871854305267334 tensor(0.9062)\n",
      "0.09127075970172882 tensor(0.9844)\n",
      "0.08387500047683716 tensor(1.)\n",
      "0.28653448820114136 tensor(0.9219)\n",
      "0.3715171217918396 tensor(0.9219)\n",
      "0.36347100138664246 tensor(0.9062)\n",
      "0.32047370076179504 tensor(0.9375)\n",
      "0.28208237886428833 tensor(0.9062)\n",
      "0.17466214299201965 tensor(0.9375)\n",
      "0.222504660487175 tensor(0.9375)\n",
      "0.19505250453948975 tensor(0.9375)\n",
      "0.21666625142097473 tensor(0.9688)\n",
      "0.096932053565979 tensor(0.9844)\n",
      "0.3308325409889221 tensor(0.8906)\n",
      "0.15741181373596191 tensor(0.9375)\n",
      "0.26425179839134216 tensor(0.9375)\n",
      "0.2651146650314331 tensor(0.9062)\n",
      "0.1693446934223175 tensor(0.9375)\n",
      "0.4604906439781189 tensor(0.9062)\n",
      "0.27722543478012085 tensor(0.9375)\n",
      "0.21642233431339264 tensor(0.9375)\n",
      "0.5488032102584839 tensor(0.8594)\n",
      "0.20150278508663177 tensor(0.9375)\n",
      "0.296256422996521 tensor(0.9219)\n",
      "0.2592388093471527 tensor(0.9375)\n",
      "0.22640261054039001 tensor(0.9375)\n",
      "0.07321066409349442 tensor(0.9844)\n",
      "0.3402281403541565 tensor(0.9062)\n",
      "0.18541713058948517 tensor(0.9219)\n",
      "0.3233017921447754 tensor(0.9062)\n",
      "0.2520858943462372 tensor(0.9219)\n",
      "0.21929679811000824 tensor(0.9062)\n",
      "0.3404689431190491 tensor(0.9375)\n",
      "0.2819463014602661 tensor(0.9219)\n",
      "0.20628035068511963 tensor(0.9688)\n",
      "0.16420692205429077 tensor(0.9531)\n",
      "0.3277684152126312 tensor(0.9219)\n",
      "0.09548360109329224 tensor(0.9844)\n",
      "0.1827889382839203 tensor(0.9531)\n",
      "0.08843259513378143 tensor(1.)\n",
      "0.1194976270198822 tensor(0.9688)\n",
      "0.2619040012359619 tensor(0.9219)\n",
      "0.3215421140193939 tensor(0.9219)\n",
      "0.14212572574615479 tensor(0.9375)\n",
      "0.18308694660663605 tensor(0.9531)\n",
      "0.17776933312416077 tensor(0.9531)\n",
      "0.14567804336547852 tensor(0.9531)\n",
      "0.36050140857696533 tensor(0.8750)\n",
      "0.44394949078559875 tensor(0.9062)\n",
      "0.43380725383758545 tensor(0.8594)\n",
      "0.5464147329330444 tensor(0.8438)\n",
      "0.1943093091249466 tensor(0.9531)\n",
      "0.31109461188316345 tensor(0.9062)\n",
      "0.32105928659439087 tensor(0.8594)\n",
      "0.3840215504169464 tensor(0.8750)\n",
      "0.39388567209243774 tensor(0.8906)\n",
      "0.1605091542005539 tensor(0.9219)\n",
      "0.1763220578432083 tensor(0.9375)\n",
      "0.2790786325931549 tensor(0.9219)\n",
      "0.21588566899299622 tensor(0.9219)\n",
      "0.25468558073043823 tensor(0.9219)\n",
      "0.21584826707839966 tensor(0.9219)\n",
      "0.2303420752286911 tensor(0.9375)\n",
      "0.47158950567245483 tensor(0.9062)\n",
      "0.3641452193260193 tensor(0.8906)\n",
      "0.19379429519176483 tensor(0.9062)\n",
      "0.1790890097618103 tensor(0.9219)\n",
      "0.34122681617736816 tensor(0.8906)\n",
      "0.14050182700157166 tensor(0.9375)\n",
      "0.5683901906013489 tensor(0.8906)\n",
      "0.2663387060165405 tensor(0.9219)\n",
      "0.15977418422698975 tensor(0.9531)\n",
      "0.24734798073768616 tensor(0.9375)\n",
      "0.5115091800689697 tensor(0.8906)\n",
      "0.23115119338035583 tensor(0.9688)\n",
      "0.2363426685333252 tensor(0.9219)\n",
      "0.4726113975048065 tensor(0.9062)\n",
      "0.7071993947029114 tensor(0.8594)\n",
      "0.4835743010044098 tensor(0.8594)\n",
      "0.5155454874038696 tensor(0.8438)\n",
      "0.34860488772392273 tensor(0.9062)\n",
      "0.21309083700180054 tensor(0.9375)\n",
      "0.13889707624912262 tensor(0.9688)\n",
      "0.36614060401916504 tensor(0.9062)\n",
      "0.22688481211662292 tensor(0.9531)\n",
      "0.3223360776901245 tensor(0.9219)\n",
      "0.28286173939704895 tensor(0.9375)\n",
      "0.31351417303085327 tensor(0.8594)\n",
      "0.4498898983001709 tensor(0.8906)\n",
      "0.2565184235572815 tensor(0.9375)\n",
      "0.3765257000923157 tensor(0.9219)\n",
      "0.21575921773910522 tensor(0.9375)\n",
      "0.06081327795982361 tensor(0.9844)\n",
      "0.3867166340351105 tensor(0.9219)\n",
      "0.059214264154434204 tensor(0.9844)\n",
      "0.11879048496484756 tensor(0.9531)\n",
      "0.22475217282772064 tensor(0.9219)\n",
      "0.3332880139350891 tensor(0.9375)\n",
      "0.16978953778743744 tensor(0.9688)\n",
      "0.2179698646068573 tensor(0.9219)\n",
      "0.49928927421569824 tensor(0.8750)\n",
      "0.16261543333530426 tensor(0.9688)\n",
      "0.22316575050354004 tensor(0.9375)\n",
      "0.12253271788358688 tensor(0.9531)\n",
      "0.1902354508638382 tensor(0.9375)\n",
      "0.06555546075105667 tensor(1.)\n",
      "0.1186322271823883 tensor(0.9688)\n",
      "0.18187355995178223 tensor(0.9688)\n",
      "0.40347185730934143 tensor(0.9062)\n",
      "0.25840187072753906 tensor(0.9219)\n",
      "0.2175670862197876 tensor(0.9531)\n",
      "0.17493034899234772 tensor(0.9531)\n",
      "0.23467442393302917 tensor(0.9531)\n",
      "0.21326623857021332 tensor(0.9375)\n",
      "0.14091387391090393 tensor(0.9531)\n",
      "0.12207513302564621 tensor(0.9688)\n",
      "0.23679549992084503 tensor(0.9062)\n",
      "0.12976637482643127 tensor(0.9531)\n",
      "0.1688268482685089 tensor(0.9375)\n",
      "0.2206002175807953 tensor(0.9219)\n",
      "0.25178131461143494 tensor(0.9375)\n",
      "0.4704499840736389 tensor(0.8438)\n",
      "0.382363498210907 tensor(0.8906)\n",
      "0.5374738574028015 tensor(0.8594)\n",
      "0.45450669527053833 tensor(0.9062)\n",
      "0.15199162065982819 tensor(0.9375)\n",
      "0.3119139075279236 tensor(0.8906)\n",
      "0.2018147110939026 tensor(0.9375)\n",
      "0.20103803277015686 tensor(0.9219)\n",
      "0.30073851346969604 tensor(0.9219)\n",
      "0.17445629835128784 tensor(0.9531)\n",
      "0.44502583146095276 tensor(0.9062)\n",
      "0.20654985308647156 tensor(0.9219)\n",
      "0.272409051656723 tensor(0.9219)\n",
      "0.30023497343063354 tensor(0.9375)\n",
      "0.2721780240535736 tensor(0.8906)\n",
      "0.38521891832351685 tensor(0.9219)\n",
      "0.6214311122894287 tensor(0.8594)\n",
      "0.6394006013870239 tensor(0.8594)\n",
      "0.46648651361465454 tensor(0.9219)\n",
      "0.30123603343963623 tensor(0.8750)\n",
      "0.17174801230430603 tensor(0.9219)\n",
      "0.2387000024318695 tensor(0.9219)\n",
      "0.48898234963417053 tensor(0.8594)\n",
      "0.5423821210861206 tensor(0.8750)\n",
      "0.43630972504615784 tensor(0.8594)\n",
      "0.3489276170730591 tensor(0.8750)\n",
      "0.2292938232421875 tensor(0.9531)\n",
      "0.2512139081954956 tensor(0.9531)\n",
      "0.1859491765499115 tensor(0.9219)\n",
      "0.29466021060943604 tensor(0.9062)\n",
      "0.18453386425971985 tensor(0.9844)\n",
      "0.3551468849182129 tensor(0.8906)\n",
      "0.10510347783565521 tensor(0.9844)\n",
      "0.4269501268863678 tensor(0.9062)\n",
      "0.38175973296165466 tensor(0.8906)\n",
      "0.17904004454612732 tensor(0.9375)\n",
      "0.3612321615219116 tensor(0.9062)\n",
      "0.243800550699234 tensor(0.9219)\n",
      "0.6061551570892334 tensor(0.8438)\n",
      "0.45100972056388855 tensor(0.8750)\n",
      "0.33876973390579224 tensor(0.9219)\n",
      "0.2819820046424866 tensor(0.8906)\n",
      "0.26534003019332886 tensor(0.8906)\n",
      "0.35244137048721313 tensor(0.8750)\n",
      "0.7582488059997559 tensor(0.8125)\n",
      "0.2820170223712921 tensor(0.9062)\n",
      "0.22012056410312653 tensor(0.9531)\n",
      "0.3702518939971924 tensor(0.8906)\n",
      "0.41093185544013977 tensor(0.8594)\n",
      "0.4128502607345581 tensor(0.8594)\n",
      "0.5854963660240173 tensor(0.8281)\n",
      "0.6437140107154846 tensor(0.8125)\n",
      "0.2229478657245636 tensor(0.9219)\n",
      "0.2143460065126419 tensor(0.9375)\n",
      "0.22069725394248962 tensor(0.9531)\n",
      "0.21752935647964478 tensor(0.9219)\n",
      "0.5132789015769958 tensor(0.8750)\n",
      "0.28558778762817383 tensor(0.9375)\n",
      "0.20636773109436035 tensor(0.9219)\n",
      "0.20496635138988495 tensor(0.9688)\n",
      "0.13904479146003723 tensor(0.9375)\n",
      "0.2265757918357849 tensor(0.9375)\n",
      "0.26349133253097534 tensor(0.9375)\n",
      "0.21205823123455048 tensor(0.9062)\n",
      "0.1543334424495697 tensor(0.9531)\n",
      "0.3238884508609772 tensor(0.9219)\n",
      "0.33471405506134033 tensor(0.8594)\n",
      "0.25116702914237976 tensor(0.8750)\n",
      "0.16704192757606506 tensor(0.9375)\n",
      "0.6060662865638733 tensor(0.8906)\n",
      "0.4634273052215576 tensor(0.8281)\n",
      "0.15629905462265015 tensor(0.9688)\n",
      "0.1498861461877823 tensor(0.9375)\n",
      "0.1771228313446045 tensor(0.9375)\n",
      "0.21314719319343567 tensor(0.9531)\n",
      "0.27140700817108154 tensor(0.9531)\n",
      "0.16657593846321106 tensor(0.9531)\n",
      "0.2695252299308777 tensor(0.9531)\n",
      "0.44139906764030457 tensor(0.9375)\n",
      "0.29642215371131897 tensor(0.9219)\n",
      "0.32735779881477356 tensor(0.8438)\n",
      "0.21414655447006226 tensor(0.9219)\n",
      "0.32784128189086914 tensor(0.8906)\n",
      "0.2224717140197754 tensor(0.9531)\n",
      "0.2521328032016754 tensor(0.9375)\n",
      "0.3023110032081604 tensor(0.8750)\n",
      "0.32021304965019226 tensor(0.9219)\n",
      "0.2649875283241272 tensor(0.9062)\n",
      "0.2845292091369629 tensor(0.9219)\n",
      "0.3964103162288666 tensor(0.8906)\n",
      "0.1283838152885437 tensor(0.9688)\n",
      "0.2835277318954468 tensor(0.9375)\n",
      "0.22555837035179138 tensor(0.9531)\n",
      "0.33827537298202515 tensor(0.8906)\n",
      "0.43620023131370544 tensor(0.8750)\n",
      "0.21566817164421082 tensor(0.9375)\n",
      "0.32348987460136414 tensor(0.9062)\n",
      "0.2846173644065857 tensor(0.9062)\n",
      "0.384006142616272 tensor(0.8594)\n",
      "0.21869081258773804 tensor(0.9062)\n",
      "0.22636520862579346 tensor(0.9375)\n",
      "0.23392659425735474 tensor(0.9219)\n",
      "0.2682805359363556 tensor(0.9375)\n",
      "0.1329936683177948 tensor(0.9688)\n",
      "0.11260342597961426 tensor(0.9688)\n",
      "0.1912415623664856 tensor(0.9688)\n",
      "0.25103914737701416 tensor(0.9531)\n",
      "0.2260100245475769 tensor(0.9062)\n",
      "0.30367234349250793 tensor(0.9062)\n",
      "0.2393927276134491 tensor(0.9375)\n",
      "0.39769914746284485 tensor(0.9219)\n",
      "0.15139523148536682 tensor(0.9844)\n",
      "0.33723828196525574 tensor(0.9375)\n",
      "0.10420793294906616 tensor(0.9688)\n",
      "0.3036368191242218 tensor(0.9219)\n",
      "0.14609664678573608 tensor(0.9688)\n",
      "0.16834068298339844 tensor(0.9531)\n",
      "0.2969033122062683 tensor(0.9062)\n",
      "0.2818891108036041 tensor(0.9531)\n",
      "0.19363877177238464 tensor(0.9375)\n",
      "0.31831395626068115 tensor(0.9219)\n",
      "0.16068394482135773 tensor(0.9531)\n",
      "0.2654039263725281 tensor(0.9219)\n",
      "0.28214654326438904 tensor(0.8906)\n",
      "0.3168167769908905 tensor(0.9375)\n",
      "0.31327611207962036 tensor(0.9062)\n",
      "0.21513551473617554 tensor(0.9531)\n",
      "0.149193674325943 tensor(0.9688)\n",
      "0.08709581196308136 tensor(0.9531)\n",
      "0.4069715142250061 tensor(0.9219)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13760054111480713 tensor(0.9375)\n",
      "0.2374996542930603 tensor(0.9219)\n",
      "0.28263261914253235 tensor(0.8906)\n",
      "0.2933351695537567 tensor(0.8906)\n",
      "0.15938973426818848 tensor(0.9688)\n",
      "0.49057066440582275 tensor(0.8594)\n",
      "0.17424814403057098 tensor(0.9375)\n",
      "0.2513473331928253 tensor(0.9531)\n",
      "0.1290832757949829 tensor(0.9531)\n",
      "0.15452763438224792 tensor(0.9375)\n",
      "0.16074267029762268 tensor(0.9531)\n",
      "0.3608434796333313 tensor(0.9062)\n",
      "0.2567233145236969 tensor(0.9375)\n",
      "0.541446328163147 tensor(0.9219)\n",
      "0.3577418327331543 tensor(0.9062)\n",
      "0.3942852020263672 tensor(0.9219)\n",
      "0.6241058111190796 tensor(0.8750)\n",
      "0.6544413566589355 tensor(0.8594)\n",
      "0.3775171935558319 tensor(0.9062)\n",
      "0.3589111566543579 tensor(0.9375)\n",
      "0.2578904330730438 tensor(0.9688)\n",
      "0.2536904811859131 tensor(0.9219)\n",
      "0.1435662806034088 tensor(0.9688)\n",
      "0.27738744020462036 tensor(0.9375)\n",
      "0.2630101442337036 tensor(0.9375)\n",
      "0.08520247042179108 tensor(0.9688)\n",
      "0.18586769700050354 tensor(0.9531)\n",
      "0.49383026361465454 tensor(0.8906)\n",
      "0.24958890676498413 tensor(0.9219)\n",
      "0.23568794131278992 tensor(0.9375)\n",
      "0.07188161462545395 tensor(1.)\n",
      "0.08077460527420044 tensor(0.9844)\n",
      "0.23578070104122162 tensor(0.9062)\n",
      "0.3335959315299988 tensor(0.9375)\n",
      "0.08564645051956177 tensor(1.)\n",
      "0.298987478017807 tensor(0.9219)\n",
      "0.38779518008232117 tensor(0.8906)\n",
      "0.4961528182029724 tensor(0.9375)\n",
      "0.2618083357810974 tensor(0.9219)\n",
      "0.060424163937568665 tensor(1.)\n",
      "0.23703184723854065 tensor(0.9219)\n",
      "0.44900238513946533 tensor(0.8750)\n",
      "0.5932744741439819 tensor(0.8281)\n",
      "0.41347536444664 tensor(0.8906)\n",
      "0.20126007497310638 tensor(0.9688)\n",
      "0.18436753749847412 tensor(0.9219)\n",
      "0.21098437905311584 tensor(0.9219)\n",
      "0.20081019401550293 tensor(0.9219)\n",
      "0.07011573016643524 tensor(0.9688)\n",
      "0.27588507533073425 tensor(0.9219)\n",
      "0.4224451780319214 tensor(0.8750)\n",
      "0.3134636878967285 tensor(0.9375)\n",
      "0.28638923168182373 tensor(0.8906)\n",
      "0.10626101493835449 tensor(0.9531)\n",
      "0.1497308313846588 tensor(0.9531)\n",
      "0.2115829735994339 tensor(0.9531)\n",
      "0.3507223129272461 tensor(0.8750)\n",
      "0.18178921937942505 tensor(0.9375)\n",
      "0.25273072719573975 tensor(0.9219)\n",
      "0.1918722689151764 tensor(0.9375)\n",
      "0.48206645250320435 tensor(0.8438)\n",
      "0.13769583404064178 tensor(0.9531)\n",
      "0.33523398637771606 tensor(0.9219)\n",
      "0.25901997089385986 tensor(0.9062)\n",
      "0.4391126036643982 tensor(0.9219)\n",
      "0.2917040288448334 tensor(0.9219)\n",
      "0.17992904782295227 tensor(0.9531)\n",
      "0.15446409583091736 tensor(0.9531)\n",
      "0.23962315917015076 tensor(0.9375)\n",
      "0.38883596658706665 tensor(0.8594)\n",
      "0.17656153440475464 tensor(0.9531)\n",
      "0.1907189041376114 tensor(0.9219)\n",
      "0.26254788041114807 tensor(0.9531)\n",
      "0.3726370930671692 tensor(0.9062)\n",
      "0.34097525477409363 tensor(0.9062)\n",
      "0.35812729597091675 tensor(0.8750)\n",
      "0.25097042322158813 tensor(0.9062)\n",
      "0.4234761893749237 tensor(0.9688)\n",
      "0.24226516485214233 tensor(0.9062)\n",
      "0.33215850591659546 tensor(0.8906)\n",
      "0.2641901969909668 tensor(0.9531)\n",
      "0.0866960883140564 tensor(0.9844)\n",
      "0.29810401797294617 tensor(0.9219)\n",
      "0.27443796396255493 tensor(0.9219)\n",
      "0.17889320850372314 tensor(0.9219)\n",
      "0.46056050062179565 tensor(0.8750)\n",
      "0.08374299108982086 tensor(0.9688)\n",
      "0.0855981856584549 tensor(0.9688)\n",
      "0.2592865824699402 tensor(0.9375)\n",
      "0.3205047845840454 tensor(0.9375)\n",
      "0.1948411464691162 tensor(0.9219)\n",
      "0.23338210582733154 tensor(0.9219)\n",
      "0.2123783826828003 tensor(0.9375)\n",
      "0.4088568687438965 tensor(0.8594)\n",
      "0.14848114550113678 tensor(0.9531)\n",
      "0.20026907324790955 tensor(0.9375)\n",
      "0.1701052337884903 tensor(0.9688)\n",
      "0.22021260857582092 tensor(0.9219)\n",
      "0.15095333755016327 tensor(0.9688)\n",
      "0.1394464671611786 tensor(0.9531)\n",
      "0.28088662028312683 tensor(0.9531)\n",
      "0.2684440016746521 tensor(0.9688)\n",
      "0.5747115015983582 tensor(0.9219)\n",
      "0.2544015944004059 tensor(0.8750)\n",
      "0.39047014713287354 tensor(0.8906)\n",
      "0.32200634479522705 tensor(0.9219)\n",
      "0.5451589226722717 tensor(0.8750)\n",
      "0.5971694588661194 tensor(0.8125)\n",
      "0.3581468164920807 tensor(0.8594)\n",
      "0.3248995542526245 tensor(0.9219)\n",
      "0.5065562725067139 tensor(0.9375)\n",
      "0.0977037325501442 tensor(0.9844)\n",
      "0.15367838740348816 tensor(0.9375)\n",
      "0.19988951086997986 tensor(0.9375)\n",
      "0.7354505658149719 tensor(0.8438)\n",
      "0.433968186378479 tensor(0.9062)\n",
      "0.13515067100524902 tensor(0.9375)\n",
      "0.23800179362297058 tensor(0.9375)\n",
      "0.23067370057106018 tensor(0.9375)\n",
      "0.317508339881897 tensor(0.9375)\n",
      "0.20807573199272156 tensor(0.9219)\n",
      "0.3227996528148651 tensor(0.8594)\n",
      "0.3231848478317261 tensor(0.9219)\n",
      "0.17444121837615967 tensor(0.9375)\n",
      "0.29777365922927856 tensor(0.8750)\n",
      "0.23184546828269958 tensor(0.9219)\n",
      "0.12927566468715668 tensor(0.9375)\n",
      "0.14175373315811157 tensor(0.9688)\n",
      "0.13012918829917908 tensor(0.9688)\n",
      "0.2506381869316101 tensor(0.9219)\n",
      "0.22795845568180084 tensor(0.9375)\n",
      "0.18896205723285675 tensor(0.9375)\n",
      "0.07228559255599976 tensor(0.9844)\n",
      "0.7146234512329102 tensor(0.8906)\n",
      "0.15739141404628754 tensor(0.9531)\n",
      "0.34262946248054504 tensor(0.9531)\n",
      "0.32249754667282104 tensor(0.9219)\n",
      "0.7597142457962036 tensor(0.8281)\n",
      "0.22013095021247864 tensor(0.9375)\n",
      "0.1832115203142166 tensor(0.9219)\n",
      "0.15211477875709534 tensor(0.9531)\n",
      "0.09360012412071228 tensor(0.9688)\n",
      "0.28218042850494385 tensor(0.8906)\n",
      "0.11362173408269882 tensor(0.9688)\n",
      "0.2627195715904236 tensor(0.9375)\n",
      "0.3502967059612274 tensor(0.8906)\n",
      "0.3525417149066925 tensor(0.9062)\n",
      "0.24796199798583984 tensor(0.9375)\n",
      "0.23392030596733093 tensor(0.9062)\n",
      "0.27626416087150574 tensor(0.9375)\n",
      "0.15467354655265808 tensor(0.9062)\n",
      "0.20630505681037903 tensor(0.9375)\n",
      "0.217185378074646 tensor(0.9219)\n",
      "0.24297744035720825 tensor(0.9219)\n",
      "0.3740350008010864 tensor(0.8750)\n",
      "0.18584081530570984 tensor(0.9219)\n",
      "0.4506448805332184 tensor(0.8906)\n",
      "0.36329418420791626 tensor(0.8750)\n",
      "0.36672550439834595 tensor(0.8906)\n",
      "0.2813807725906372 tensor(0.8750)\n",
      "0.34824544191360474 tensor(0.8750)\n",
      "0.36528199911117554 tensor(0.9219)\n",
      "0.3203728497028351 tensor(0.8906)\n",
      "0.1951267123222351 tensor(0.9375)\n",
      "0.15477515757083893 tensor(0.9688)\n",
      "0.2179143726825714 tensor(0.9375)\n",
      "0.31512317061424255 tensor(0.8594)\n",
      "0.2337830364704132 tensor(0.9219)\n",
      "0.3732524514198303 tensor(0.8594)\n",
      "0.29082798957824707 tensor(0.9062)\n",
      "0.3203703463077545 tensor(0.9062)\n",
      "0.2568293809890747 tensor(0.8750)\n",
      "0.37581726908683777 tensor(0.9062)\n",
      "0.21719321608543396 tensor(0.9375)\n",
      "0.3430859446525574 tensor(0.8906)\n",
      "0.34585025906562805 tensor(0.9375)\n",
      "0.4954160749912262 tensor(0.8750)\n",
      "0.5557740926742554 tensor(0.9062)\n",
      "0.30096739530563354 tensor(0.9062)\n",
      "0.669682502746582 tensor(0.8594)\n",
      "0.4668891429901123 tensor(0.8438)\n",
      "0.3602938652038574 tensor(0.8750)\n",
      "0.13494080305099487 tensor(0.9688)\n",
      "0.27607467770576477 tensor(0.9219)\n",
      "0.3900701403617859 tensor(0.8750)\n",
      "0.5916406512260437 tensor(0.8438)\n",
      "0.3269799053668976 tensor(0.8906)\n",
      "0.26212722063064575 tensor(0.9219)\n",
      "0.24194636940956116 tensor(0.9375)\n",
      "0.31282031536102295 tensor(0.8750)\n",
      "0.3365173935890198 tensor(0.8906)\n",
      "0.271604061126709 tensor(0.9062)\n",
      "0.4153226912021637 tensor(0.8750)\n",
      "0.2734471559524536 tensor(0.9219)\n",
      "0.41449448466300964 tensor(0.9062)\n",
      "0.4124911427497864 tensor(0.8750)\n",
      "0.41308659315109253 tensor(0.8594)\n",
      "0.33398178219795227 tensor(0.9375)\n",
      "0.1808224469423294 tensor(0.9531)\n",
      "0.08234104514122009 tensor(0.9688)\n",
      "0.47363346815109253 tensor(0.8750)\n",
      "0.2312372624874115 tensor(0.9375)\n",
      "0.4802865982055664 tensor(0.8906)\n",
      "0.23077160120010376 tensor(0.9688)\n",
      "0.34345853328704834 tensor(0.9219)\n",
      "0.16775836050510406 tensor(0.9219)\n",
      "0.24511684477329254 tensor(0.9375)\n",
      "0.17259961366653442 tensor(0.9375)\n",
      "0.16408827900886536 tensor(0.9531)\n",
      "0.24495162069797516 tensor(0.9531)\n",
      "0.4374891519546509 tensor(0.9219)\n",
      "0.3233639597892761 tensor(0.9219)\n",
      "0.2958025634288788 tensor(0.8750)\n",
      "0.2531735301017761 tensor(0.9219)\n",
      "0.23196646571159363 tensor(0.9062)\n",
      "0.1409452110528946 tensor(0.9531)\n",
      "0.14751021564006805 tensor(0.9531)\n",
      "0.32942068576812744 tensor(0.9219)\n",
      "0.09276556968688965 tensor(0.9688)\n",
      "0.07891979068517685 tensor(0.9688)\n",
      "0.1334579586982727 tensor(0.9375)\n",
      "0.17261746525764465 tensor(0.9688)\n",
      "0.24575962126255035 tensor(0.9219)\n",
      "0.15004998445510864 tensor(0.9688)\n",
      "0.16351664066314697 tensor(0.9062)\n",
      "0.15476682782173157 tensor(0.9688)\n",
      "0.14569230377674103 tensor(0.9531)\n",
      "0.31861788034439087 tensor(0.9062)\n",
      "0.37590470910072327 tensor(0.8906)\n",
      "0.4621250629425049 tensor(0.8594)\n",
      "0.24614253640174866 tensor(0.8906)\n",
      "0.4029369354248047 tensor(0.9062)\n",
      "0.32565680146217346 tensor(0.9219)\n",
      "0.34250012040138245 tensor(0.8906)\n",
      "0.44023382663726807 tensor(0.8750)\n",
      "0.25757914781570435 tensor(0.8906)\n",
      "0.1408553123474121 tensor(0.9531)\n",
      "0.3043387830257416 tensor(0.9375)\n",
      "0.15642297267913818 tensor(0.9531)\n",
      "0.2411218136548996 tensor(0.9375)\n",
      "0.3007078170776367 tensor(0.9219)\n",
      "0.15232670307159424 tensor(0.9375)\n",
      "0.17651282250881195 tensor(0.9531)\n",
      "0.2532438039779663 tensor(0.9375)\n",
      "0.4353753924369812 tensor(0.8594)\n",
      "0.15108895301818848 tensor(0.9531)\n",
      "0.2846735119819641 tensor(0.9219)\n",
      "0.193305104970932 tensor(0.9531)\n",
      "0.1501053422689438 tensor(0.9844)\n",
      "0.11091967672109604 tensor(0.9531)\n",
      "0.20931604504585266 tensor(0.9219)\n",
      "0.2587490975856781 tensor(0.9219)\n",
      "0.25052422285079956 tensor(0.9219)\n",
      "0.3415633738040924 tensor(0.9219)\n",
      "0.2598971724510193 tensor(0.9375)\n",
      "0.23309382796287537 tensor(0.9375)\n",
      "0.25301632285118103 tensor(0.9375)\n",
      "0.08378913253545761 tensor(0.9688)\n",
      "0.26874083280563354 tensor(0.9219)\n",
      "0.37988030910491943 tensor(0.9062)\n",
      "0.16244013607501984 tensor(0.9375)\n",
      "0.2541205585002899 tensor(0.9219)\n",
      "0.1216345727443695 tensor(0.9844)\n",
      "0.1693873107433319 tensor(0.9844)\n",
      "0.3015449643135071 tensor(0.9219)\n",
      "0.265354186296463 tensor(0.9062)\n",
      "0.2424732744693756 tensor(0.9219)\n",
      "0.13859842717647552 tensor(0.9688)\n",
      "0.25331029295921326 tensor(0.9219)\n",
      "0.4244416058063507 tensor(0.8750)\n",
      "0.4418312609195709 tensor(0.8438)\n",
      "0.19311939179897308 tensor(0.9219)\n",
      "0.527725338935852 tensor(0.8438)\n",
      "0.6673261523246765 tensor(0.8750)\n",
      "0.9260592460632324 tensor(0.7969)\n",
      "0.5374086499214172 tensor(0.8438)\n",
      "0.43151435256004333 tensor(0.8281)\n",
      "0.17387719452381134 tensor(0.9688)\n",
      "0.26640307903289795 tensor(0.9062)\n",
      "0.4847591519355774 tensor(0.8750)\n",
      "0.2669670283794403 tensor(0.9219)\n",
      "0.32626110315322876 tensor(0.9531)\n",
      "0.2580485939979553 tensor(0.9375)\n",
      "0.40456876158714294 tensor(0.9219)\n",
      "0.24837376177310944 tensor(0.9219)\n",
      "0.15619148313999176 tensor(0.9531)\n",
      "0.12503007054328918 tensor(0.9688)\n",
      "0.34224605560302734 tensor(0.8750)\n",
      "0.3104930520057678 tensor(0.8906)\n",
      "0.3202085494995117 tensor(0.8750)\n",
      "0.19822756946086884 tensor(0.9219)\n",
      "0.31982916593551636 tensor(0.8906)\n",
      "0.3452772796154022 tensor(0.9219)\n",
      "0.4323852062225342 tensor(0.8750)\n",
      "0.3331984877586365 tensor(0.8281)\n",
      "0.26594001054763794 tensor(0.9219)\n",
      "0.1236647367477417 tensor(0.9531)\n",
      "0.12440896779298782 tensor(0.9688)\n",
      "0.17049293220043182 tensor(0.9531)\n",
      "0.1809220314025879 tensor(0.9688)\n",
      "0.14956101775169373 tensor(0.9844)\n",
      "0.26103904843330383 tensor(0.9062)\n",
      "0.20286226272583008 tensor(0.9375)\n",
      "0.24792325496673584 tensor(0.8750)\n",
      "0.7942215204238892 tensor(0.7969)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40394705533981323 tensor(0.8750)\n",
      "0.4632372260093689 tensor(0.8750)\n",
      "0.2459852546453476 tensor(0.9375)\n",
      "0.17709098756313324 tensor(0.9375)\n",
      "0.28903186321258545 tensor(0.9219)\n",
      "0.2390616536140442 tensor(0.9688)\n",
      "0.29209211468696594 tensor(0.8750)\n",
      "0.19383063912391663 tensor(0.9531)\n",
      "0.21446886658668518 tensor(0.9531)\n",
      "0.42747190594673157 tensor(0.9062)\n",
      "0.1827058494091034 tensor(0.9219)\n",
      "0.33314478397369385 tensor(0.8906)\n",
      "0.24160519242286682 tensor(0.9062)\n",
      "0.14492271840572357 tensor(0.9688)\n",
      "0.2631601393222809 tensor(0.9062)\n",
      "0.283083975315094 tensor(0.9219)\n",
      "0.167182058095932 tensor(0.9688)\n",
      "0.34534966945648193 tensor(0.8906)\n",
      "0.28744009137153625 tensor(0.9062)\n",
      "0.24376174807548523 tensor(0.9062)\n",
      "0.2426968812942505 tensor(0.9375)\n",
      "0.35960057377815247 tensor(0.9375)\n",
      "0.15708842873573303 tensor(0.9375)\n",
      "0.17667913436889648 tensor(0.9531)\n",
      "0.18972060084342957 tensor(0.9531)\n",
      "0.4765377640724182 tensor(0.8438)\n",
      "0.2460229992866516 tensor(0.9375)\n",
      "0.25818488001823425 tensor(0.9219)\n",
      "0.22892791032791138 tensor(0.9219)\n",
      "0.3201735317707062 tensor(0.9062)\n",
      "0.48666560649871826 tensor(0.8594)\n",
      "0.2900988757610321 tensor(0.8594)\n",
      "0.6005390882492065 tensor(0.8750)\n",
      "0.353681355714798 tensor(0.9219)\n",
      "0.3622751832008362 tensor(0.8906)\n",
      "0.1744794249534607 tensor(0.9219)\n",
      "0.17468681931495667 tensor(0.9375)\n",
      "0.24208104610443115 tensor(0.9531)\n",
      "0.2014942765235901 tensor(0.9062)\n",
      "0.17771853506565094 tensor(0.9375)\n",
      "0.3029019832611084 tensor(0.9062)\n",
      "0.4774482250213623 tensor(0.9062)\n",
      "0.2124721109867096 tensor(0.9375)\n",
      "0.3077125549316406 tensor(0.9531)\n",
      "0.2878427505493164 tensor(0.9062)\n",
      "0.13968665897846222 tensor(0.9688)\n",
      "0.6067163348197937 tensor(0.8438)\n",
      "0.27967748045921326 tensor(0.9375)\n",
      "0.6630603671073914 tensor(0.8906)\n",
      "0.612960934638977 tensor(0.8750)\n",
      "0.14094601571559906 tensor(0.9688)\n",
      "0.21675875782966614 tensor(0.9219)\n",
      "0.32707831263542175 tensor(0.8906)\n",
      "0.34150180220603943 tensor(0.9062)\n",
      "0.399486243724823 tensor(0.8906)\n",
      "0.3876142203807831 tensor(0.8594)\n",
      "0.36054477095603943 tensor(0.8750)\n",
      "0.24566127359867096 tensor(0.9062)\n",
      "0.2696637511253357 tensor(0.8594)\n",
      "0.28686991333961487 tensor(0.8906)\n",
      "0.4321472942829132 tensor(0.8906)\n",
      "0.0652683824300766 tensor(0.9844)\n",
      "0.22026732563972473 tensor(0.9219)\n",
      "0.11780380457639694 tensor(0.9688)\n",
      "0.13848723471164703 tensor(0.9688)\n",
      "0.3355562090873718 tensor(0.9219)\n",
      "0.08826769888401031 tensor(0.9844)\n",
      "0.343315988779068 tensor(0.9062)\n",
      "0.09765570610761642 tensor(0.9688)\n",
      "0.2300351858139038 tensor(0.9375)\n",
      "0.3566155731678009 tensor(0.9062)\n",
      "0.3666372299194336 tensor(0.9219)\n",
      "0.25251927971839905 tensor(0.9219)\n",
      "0.16711312532424927 tensor(0.9375)\n",
      "0.37392741441726685 tensor(0.8906)\n",
      "0.14628437161445618 tensor(0.9844)\n",
      "0.3069009780883789 tensor(0.9062)\n",
      "0.37198084592819214 tensor(0.8750)\n",
      "0.3407321572303772 tensor(0.9219)\n",
      "0.4582274854183197 tensor(0.9062)\n",
      "0.2128836214542389 tensor(0.9531)\n",
      "0.163492813706398 tensor(0.9375)\n",
      "0.16047906875610352 tensor(0.9531)\n",
      "0.17319482564926147 tensor(0.9688)\n",
      "0.1990595906972885 tensor(0.9375)\n",
      "0.3503507375717163 tensor(0.8906)\n",
      "0.4996279180049896 tensor(0.8594)\n",
      "0.29907503724098206 tensor(0.9219)\n",
      "0.46668192744255066 tensor(0.8906)\n",
      "0.4026666581630707 tensor(0.9062)\n",
      "0.3328803777694702 tensor(0.8906)\n",
      "0.22284719347953796 tensor(0.9375)\n",
      "0.16306912899017334 tensor(0.9219)\n",
      "0.06913623213768005 tensor(1.)\n",
      "0.3027595281600952 tensor(0.9219)\n",
      "0.5596722364425659 tensor(0.8125)\n",
      "0.27693358063697815 tensor(0.9375)\n",
      "0.4676864743232727 tensor(0.8906)\n",
      "0.21091777086257935 tensor(0.9219)\n",
      "0.16538894176483154 tensor(0.9531)\n",
      "0.504906952381134 tensor(0.9219)\n",
      "0.3242228329181671 tensor(0.8906)\n",
      "0.3353029787540436 tensor(0.9062)\n",
      "0.28292906284332275 tensor(0.8906)\n",
      "0.3093758523464203 tensor(0.8906)\n",
      "0.42632704973220825 tensor(0.8750)\n",
      "0.22875811159610748 tensor(0.9375)\n",
      "0.5102040767669678 tensor(0.8906)\n",
      "0.6056708693504333 tensor(0.8750)\n",
      "0.3441796898841858 tensor(0.8750)\n",
      "0.44944068789482117 tensor(0.8750)\n",
      "0.11100316047668457 tensor(0.9531)\n",
      "0.12760892510414124 tensor(0.9688)\n",
      "0.22772791981697083 tensor(0.9062)\n",
      "0.0986042246222496 tensor(0.9844)\n",
      "0.4385434091091156 tensor(0.9062)\n",
      "0.07818752527236938 tensor(0.9844)\n",
      "0.21371065080165863 tensor(0.9062)\n",
      "0.2266242504119873 tensor(0.9219)\n",
      "0.2662193775177002 tensor(0.9219)\n",
      "0.1725948452949524 tensor(0.9531)\n",
      "0.20146209001541138 tensor(0.9375)\n",
      "0.2635776400566101 tensor(0.9219)\n",
      "0.4134490489959717 tensor(0.8281)\n",
      "0.26140105724334717 tensor(0.9531)\n",
      "0.2393430918455124 tensor(0.9219)\n",
      "0.3990457355976105 tensor(0.8750)\n",
      "0.12793661653995514 tensor(0.9688)\n",
      "0.6264117360115051 tensor(0.8438)\n",
      "0.17447258532047272 tensor(0.9375)\n",
      "0.358989417552948 tensor(0.9219)\n",
      "0.26660558581352234 tensor(0.9375)\n",
      "0.16356267035007477 tensor(0.9531)\n",
      "0.35340940952301025 tensor(0.8906)\n",
      "0.4346015751361847 tensor(0.8906)\n",
      "0.16344690322875977 tensor(0.9688)\n",
      "0.2528407573699951 tensor(0.9531)\n",
      "0.08781223744153976 tensor(0.9844)\n",
      "0.09378978610038757 tensor(0.9688)\n",
      "0.15890981256961823 tensor(0.9531)\n",
      "0.5752299427986145 tensor(0.8125)\n",
      "0.17560280859470367 tensor(0.9531)\n",
      "0.15079215168952942 tensor(0.9688)\n",
      "0.244609534740448 tensor(0.9375)\n",
      "0.16925817728042603 tensor(0.9375)\n",
      "0.283674955368042 tensor(0.9219)\n",
      "0.15258446335792542 tensor(0.9531)\n",
      "0.14941759407520294 tensor(0.9375)\n",
      "0.20720037817955017 tensor(0.8906)\n",
      "0.5372602343559265 tensor(0.8750)\n",
      "0.6586581468582153 tensor(0.8281)\n",
      "0.35713282227516174 tensor(0.8906)\n",
      "0.1815173327922821 tensor(0.9219)\n",
      "0.3624507188796997 tensor(0.8750)\n",
      "0.2345738708972931 tensor(0.9375)\n",
      "0.19421525299549103 tensor(0.9688)\n",
      "0.09758755564689636 tensor(0.9688)\n",
      "0.2407219111919403 tensor(0.9219)\n",
      "0.6543093323707581 tensor(0.8125)\n",
      "0.5472785234451294 tensor(0.8281)\n",
      "0.5294548273086548 tensor(0.8438)\n",
      "0.23601174354553223 tensor(0.9062)\n",
      "0.193068265914917 tensor(0.9844)\n",
      "0.29487061500549316 tensor(0.9219)\n",
      "0.6106158494949341 tensor(0.8594)\n",
      "0.18427911400794983 tensor(0.9375)\n",
      "0.21893076598644257 tensor(0.9375)\n",
      "0.18336772918701172 tensor(0.9531)\n",
      "0.19345322251319885 tensor(0.9844)\n",
      "0.3606851398944855 tensor(0.9062)\n",
      "0.3061879277229309 tensor(0.8906)\n",
      "0.36846861243247986 tensor(0.8750)\n",
      "0.07406206429004669 tensor(0.9688)\n",
      "0.17266808450222015 tensor(0.9219)\n",
      "0.43586182594299316 tensor(0.9062)\n",
      "0.16394655406475067 tensor(0.9375)\n",
      "0.3298024833202362 tensor(0.9219)\n",
      "0.3143206238746643 tensor(0.9531)\n",
      "0.18257884681224823 tensor(0.9375)\n",
      "0.2699896991252899 tensor(0.9062)\n",
      "0.50926274061203 tensor(0.8594)\n",
      "0.40248289704322815 tensor(0.8906)\n",
      "0.26587000489234924 tensor(0.9219)\n",
      "0.6240918636322021 tensor(0.8438)\n",
      "0.6436298489570618 tensor(0.7812)\n",
      "0.07812672853469849 tensor(0.9844)\n",
      "0.5700152516365051 tensor(0.8750)\n",
      "0.3440403342247009 tensor(0.8906)\n",
      "0.6465576887130737 tensor(0.8594)\n",
      "0.219651997089386 tensor(0.9062)\n",
      "0.28051480650901794 tensor(0.8906)\n",
      "0.29831647872924805 tensor(0.9062)\n",
      "0.14530378580093384 tensor(0.9688)\n",
      "0.14994347095489502 tensor(0.9688)\n",
      "0.16166962683200836 tensor(0.9531)\n",
      "0.21904152631759644 tensor(0.9688)\n",
      "0.08391351997852325 tensor(0.9844)\n",
      "0.1910051703453064 tensor(0.9219)\n",
      "0.21561738848686218 tensor(0.9531)\n",
      "0.28687602281570435 tensor(0.9375)\n",
      "0.15605342388153076 tensor(0.9531)\n",
      "0.28392359614372253 tensor(0.8906)\n",
      "0.13436885178089142 tensor(0.9219)\n",
      "0.1411530077457428 tensor(0.9688)\n",
      "0.30117934942245483 tensor(0.9219)\n",
      "0.10471942275762558 tensor(0.9844)\n",
      "0.09023267030715942 tensor(0.9844)\n",
      "0.17692425847053528 tensor(0.9375)\n",
      "0.4350711405277252 tensor(0.9219)\n",
      "0.21071259677410126 tensor(0.9375)\n",
      "0.3045039176940918 tensor(0.9062)\n",
      "0.20310673117637634 tensor(0.9375)\n",
      "0.18612314760684967 tensor(0.9531)\n",
      "0.2315259575843811 tensor(0.9219)\n",
      "0.37184566259384155 tensor(0.8906)\n",
      "0.21940740942955017 tensor(0.9219)\n",
      "0.11683270335197449 tensor(0.9688)\n",
      "0.31227532029151917 tensor(0.9062)\n",
      "0.24712520837783813 tensor(0.9219)\n",
      "0.16441044211387634 tensor(0.9375)\n",
      "0.15451782941818237 tensor(0.9688)\n",
      "0.2526666224002838 tensor(0.9219)\n",
      "0.22112827003002167 tensor(0.9375)\n",
      "0.09203656017780304 tensor(0.9688)\n",
      "0.4484206736087799 tensor(0.8750)\n",
      "0.34896016120910645 tensor(0.8906)\n",
      "0.2759574055671692 tensor(0.9219)\n",
      "0.08771581947803497 tensor(0.9844)\n",
      "0.08078531920909882 tensor(0.9844)\n",
      "0.2843606472015381 tensor(0.9219)\n",
      "0.3634319305419922 tensor(0.9219)\n",
      "0.35592252016067505 tensor(0.9062)\n",
      "0.31746014952659607 tensor(0.9375)\n",
      "0.2774808704853058 tensor(0.9062)\n",
      "0.17208562791347504 tensor(0.9375)\n",
      "0.21830949187278748 tensor(0.9375)\n",
      "0.1940712034702301 tensor(0.9375)\n",
      "0.21237695217132568 tensor(0.9688)\n",
      "0.09472546726465225 tensor(0.9844)\n",
      "0.3249903619289398 tensor(0.8750)\n",
      "0.14936915040016174 tensor(0.9375)\n",
      "0.26019322872161865 tensor(0.9375)\n",
      "0.26033926010131836 tensor(0.9219)\n",
      "0.16746312379837036 tensor(0.9375)\n",
      "0.455138623714447 tensor(0.9062)\n",
      "0.27580660581588745 tensor(0.9375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21250559389591217 tensor(0.9219)\n",
      "0.5360432863235474 tensor(0.8750)\n",
      "0.20031480491161346 tensor(0.9375)\n",
      "0.2882480025291443 tensor(0.9219)\n",
      "0.2600603997707367 tensor(0.9375)\n",
      "0.2260739505290985 tensor(0.9375)\n",
      "0.07087791711091995 tensor(0.9844)\n",
      "0.3380078673362732 tensor(0.9062)\n",
      "0.17646007239818573 tensor(0.9219)\n",
      "0.32406169176101685 tensor(0.9062)\n",
      "0.24555949866771698 tensor(0.9219)\n",
      "0.22270600497722626 tensor(0.9062)\n",
      "0.33889755606651306 tensor(0.9375)\n",
      "0.2837350070476532 tensor(0.9219)\n",
      "0.20154744386672974 tensor(0.9688)\n",
      "0.1670442372560501 tensor(0.9531)\n",
      "0.32707107067108154 tensor(0.9219)\n",
      "0.09167852252721786 tensor(0.9844)\n",
      "0.1823389083147049 tensor(0.9531)\n",
      "0.0840853899717331 tensor(1.)\n",
      "0.11664815992116928 tensor(0.9688)\n",
      "0.26254695653915405 tensor(0.9219)\n",
      "0.3134908080101013 tensor(0.9219)\n",
      "0.13824008405208588 tensor(0.9375)\n",
      "0.1799473613500595 tensor(0.9531)\n",
      "0.1738457977771759 tensor(0.9531)\n",
      "0.14308342337608337 tensor(0.9531)\n",
      "0.35641416907310486 tensor(0.8750)\n",
      "0.4478415548801422 tensor(0.9062)\n",
      "0.432711124420166 tensor(0.8594)\n",
      "0.5395889282226562 tensor(0.8438)\n",
      "0.19054383039474487 tensor(0.9531)\n",
      "0.30561816692352295 tensor(0.9062)\n",
      "0.31442511081695557 tensor(0.8750)\n",
      "0.3799719512462616 tensor(0.8594)\n",
      "0.3840973377227783 tensor(0.8906)\n",
      "0.15353764593601227 tensor(0.9375)\n",
      "0.17436328530311584 tensor(0.9531)\n",
      "0.27506038546562195 tensor(0.9219)\n",
      "0.21681836247444153 tensor(0.9219)\n",
      "0.24845032393932343 tensor(0.9219)\n",
      "0.21316680312156677 tensor(0.9219)\n",
      "0.22568777203559875 tensor(0.9375)\n",
      "0.4711882472038269 tensor(0.9062)\n",
      "0.3575083017349243 tensor(0.8906)\n",
      "0.1856604814529419 tensor(0.9062)\n",
      "0.17390823364257812 tensor(0.9219)\n",
      "0.3315967619419098 tensor(0.8906)\n",
      "0.13600090146064758 tensor(0.9375)\n",
      "0.5687288045883179 tensor(0.8906)\n",
      "0.25777870416641235 tensor(0.9219)\n",
      "0.15845859050750732 tensor(0.9531)\n",
      "0.24072720110416412 tensor(0.9375)\n",
      "0.5055105686187744 tensor(0.8906)\n",
      "0.22762934863567352 tensor(0.9688)\n",
      "0.23528198897838593 tensor(0.9219)\n",
      "0.46243906021118164 tensor(0.9062)\n",
      "0.6937450170516968 tensor(0.8594)\n",
      "0.47820013761520386 tensor(0.8594)\n",
      "0.5084824562072754 tensor(0.8438)\n",
      "0.3445952534675598 tensor(0.9062)\n",
      "0.21050482988357544 tensor(0.9219)\n",
      "0.13874752819538116 tensor(0.9688)\n",
      "0.35691723227500916 tensor(0.9062)\n",
      "0.22590501606464386 tensor(0.9531)\n",
      "0.31083738803863525 tensor(0.9219)\n",
      "0.2766856551170349 tensor(0.9375)\n",
      "0.30944934487342834 tensor(0.8594)\n",
      "0.4413520097732544 tensor(0.8906)\n",
      "0.25550734996795654 tensor(0.9375)\n",
      "0.37524718046188354 tensor(0.9219)\n",
      "0.2180347442626953 tensor(0.9375)\n",
      "0.057590141892433167 tensor(0.9844)\n",
      "0.3777519166469574 tensor(0.9219)\n",
      "0.05521240085363388 tensor(0.9844)\n",
      "0.11229705065488815 tensor(0.9531)\n",
      "0.22020861506462097 tensor(0.9219)\n",
      "0.3321143090724945 tensor(0.9375)\n",
      "0.167150616645813 tensor(0.9688)\n",
      "0.21548235416412354 tensor(0.9219)\n",
      "0.49611055850982666 tensor(0.8750)\n",
      "0.15255853533744812 tensor(0.9688)\n",
      "0.2239246666431427 tensor(0.9531)\n",
      "0.12033171951770782 tensor(0.9531)\n",
      "0.18820668756961823 tensor(0.9375)\n",
      "0.061921313405036926 tensor(1.)\n",
      "0.11872117966413498 tensor(0.9688)\n",
      "0.17746122181415558 tensor(0.9844)\n",
      "0.4024428427219391 tensor(0.9062)\n",
      "0.254196435213089 tensor(0.9375)\n",
      "0.21256908774375916 tensor(0.9531)\n",
      "0.16949798166751862 tensor(0.9531)\n",
      "0.22996127605438232 tensor(0.9531)\n",
      "0.20935052633285522 tensor(0.9531)\n",
      "0.14012105762958527 tensor(0.9531)\n",
      "0.12292193621397018 tensor(0.9531)\n",
      "0.22540414333343506 tensor(0.9062)\n",
      "0.1251014769077301 tensor(0.9531)\n",
      "0.167079895734787 tensor(0.9375)\n",
      "0.21719439327716827 tensor(0.9219)\n",
      "0.24680179357528687 tensor(0.9375)\n",
      "0.45744216442108154 tensor(0.8438)\n",
      "0.37756842374801636 tensor(0.8906)\n",
      "0.5331995487213135 tensor(0.8594)\n",
      "0.44982999563217163 tensor(0.9062)\n",
      "0.14780597388744354 tensor(0.9375)\n",
      "0.3123513162136078 tensor(0.8906)\n",
      "0.20174136757850647 tensor(0.9375)\n",
      "0.19852600991725922 tensor(0.9219)\n",
      "0.2984691262245178 tensor(0.9219)\n",
      "0.16880570352077484 tensor(0.9531)\n",
      "0.44162261486053467 tensor(0.9062)\n",
      "0.20250248908996582 tensor(0.9375)\n",
      "0.2676160931587219 tensor(0.9219)\n",
      "0.2960844933986664 tensor(0.9375)\n",
      "0.26993048191070557 tensor(0.8906)\n",
      "0.3838929235935211 tensor(0.9219)\n",
      "0.6225893497467041 tensor(0.8594)\n",
      "0.6364990472793579 tensor(0.8594)\n",
      "0.4594016373157501 tensor(0.9219)\n",
      "0.29566890001296997 tensor(0.8906)\n",
      "0.16901302337646484 tensor(0.9219)\n",
      "0.23689769208431244 tensor(0.9219)\n",
      "0.47919970750808716 tensor(0.8594)\n",
      "0.5362540483474731 tensor(0.8750)\n",
      "0.43335121870040894 tensor(0.8594)\n",
      "0.34096503257751465 tensor(0.8750)\n",
      "0.22769011557102203 tensor(0.9531)\n",
      "0.24449938535690308 tensor(0.9531)\n",
      "0.17640604078769684 tensor(0.9375)\n",
      "0.2905825972557068 tensor(0.9219)\n",
      "0.18281111121177673 tensor(0.9844)\n",
      "0.35198158025741577 tensor(0.8906)\n",
      "0.1013893261551857 tensor(0.9844)\n",
      "0.4248141050338745 tensor(0.9062)\n",
      "0.37653040885925293 tensor(0.8906)\n",
      "0.17501114308834076 tensor(0.9219)\n",
      "0.35729289054870605 tensor(0.9062)\n",
      "0.2390403002500534 tensor(0.9375)\n",
      "0.6025946736335754 tensor(0.8438)\n",
      "0.44394195079803467 tensor(0.8750)\n",
      "0.3341197669506073 tensor(0.9219)\n",
      "0.2733636796474457 tensor(0.8906)\n",
      "0.25451016426086426 tensor(0.8906)\n",
      "0.3499062657356262 tensor(0.8750)\n",
      "0.7546420693397522 tensor(0.8125)\n",
      "0.2733785808086395 tensor(0.9062)\n",
      "0.21450956165790558 tensor(0.9531)\n",
      "0.3665052354335785 tensor(0.9062)\n",
      "0.40878891944885254 tensor(0.8594)\n",
      "0.4115031957626343 tensor(0.8594)\n",
      "0.586872935295105 tensor(0.8281)\n",
      "0.6327701210975647 tensor(0.8125)\n",
      "0.21901749074459076 tensor(0.9219)\n",
      "0.2123735398054123 tensor(0.9219)\n",
      "0.22283536195755005 tensor(0.9531)\n",
      "0.2166329324245453 tensor(0.9219)\n",
      "0.5060470104217529 tensor(0.8594)\n",
      "0.28255710005760193 tensor(0.9375)\n",
      "0.19835802912712097 tensor(0.9375)\n",
      "0.20278768241405487 tensor(0.9688)\n",
      "0.1328917145729065 tensor(0.9375)\n",
      "0.22168739140033722 tensor(0.9375)\n",
      "0.25557461380958557 tensor(0.9375)\n",
      "0.20899006724357605 tensor(0.9062)\n",
      "0.14730530977249146 tensor(0.9531)\n",
      "0.32210657000541687 tensor(0.9219)\n",
      "0.33122655749320984 tensor(0.8438)\n",
      "0.24793079495429993 tensor(0.8750)\n",
      "0.1595052033662796 tensor(0.9531)\n",
      "0.6056579351425171 tensor(0.8906)\n",
      "0.4625406265258789 tensor(0.8281)\n",
      "0.14895346760749817 tensor(0.9844)\n",
      "0.14971289038658142 tensor(0.9375)\n",
      "0.16885079443454742 tensor(0.9531)\n",
      "0.20299142599105835 tensor(0.9531)\n",
      "0.26989850401878357 tensor(0.9531)\n",
      "0.1647326797246933 tensor(0.9531)\n",
      "0.26568031311035156 tensor(0.9531)\n",
      "0.4377642273902893 tensor(0.9375)\n",
      "0.29448214173316956 tensor(0.9219)\n",
      "0.3240267038345337 tensor(0.8594)\n",
      "0.20751410722732544 tensor(0.9219)\n",
      "0.3191492259502411 tensor(0.8906)\n",
      "0.2188316136598587 tensor(0.9375)\n",
      "0.25135868787765503 tensor(0.9375)\n",
      "0.29122549295425415 tensor(0.8750)\n",
      "0.31595009565353394 tensor(0.9219)\n",
      "0.26069197058677673 tensor(0.9062)\n",
      "0.27871188521385193 tensor(0.9219)\n",
      "0.3964976370334625 tensor(0.8906)\n",
      "0.1274448037147522 tensor(0.9688)\n",
      "0.2802814841270447 tensor(0.9375)\n",
      "0.2226412147283554 tensor(0.9531)\n",
      "0.32913529872894287 tensor(0.8906)\n",
      "0.4241224229335785 tensor(0.8750)\n",
      "0.2120416760444641 tensor(0.9375)\n",
      "0.3192659914493561 tensor(0.9062)\n",
      "0.2815200984477997 tensor(0.9062)\n",
      "0.36855173110961914 tensor(0.8750)\n",
      "0.21200883388519287 tensor(0.9062)\n",
      "0.22499266266822815 tensor(0.9375)\n",
      "0.23182708024978638 tensor(0.9219)\n",
      "0.2652081549167633 tensor(0.9375)\n",
      "0.13124096393585205 tensor(0.9688)\n",
      "0.10966546833515167 tensor(0.9688)\n",
      "0.18727585673332214 tensor(0.9688)\n",
      "0.25119712948799133 tensor(0.9531)\n",
      "0.2178313136100769 tensor(0.9219)\n",
      "0.30260929465293884 tensor(0.9062)\n",
      "0.23708881437778473 tensor(0.9375)\n",
      "0.39409759640693665 tensor(0.9219)\n",
      "0.14548563957214355 tensor(0.9844)\n",
      "0.33112189173698425 tensor(0.9375)\n",
      "0.09965780377388 tensor(1.)\n",
      "0.3003407120704651 tensor(0.9219)\n",
      "0.14611424505710602 tensor(0.9688)\n",
      "0.16731421649456024 tensor(0.9531)\n",
      "0.29403093457221985 tensor(0.9062)\n",
      "0.2774169445037842 tensor(0.9531)\n",
      "0.19031348824501038 tensor(0.9375)\n",
      "0.3142462968826294 tensor(0.9219)\n",
      "0.15703313052654266 tensor(0.9531)\n",
      "0.26450949907302856 tensor(0.9219)\n",
      "0.28090614080429077 tensor(0.8906)\n",
      "0.31610605120658875 tensor(0.9375)\n",
      "0.30849477648735046 tensor(0.9062)\n",
      "0.2132590115070343 tensor(0.9688)\n",
      "0.15333636105060577 tensor(0.9688)\n",
      "0.085858054459095 tensor(0.9531)\n",
      "0.4045572280883789 tensor(0.9219)\n",
      "0.13699480891227722 tensor(0.9531)\n",
      "0.23265419900417328 tensor(0.9219)\n",
      "0.2780570983886719 tensor(0.8906)\n",
      "0.2860136330127716 tensor(0.8906)\n",
      "0.15527969598770142 tensor(0.9688)\n",
      "0.4856305420398712 tensor(0.8594)\n",
      "0.16943785548210144 tensor(0.9375)\n",
      "0.25023597478866577 tensor(0.9375)\n",
      "0.12410056591033936 tensor(0.9531)\n",
      "0.1485251486301422 tensor(0.9375)\n",
      "0.15624867379665375 tensor(0.9688)\n",
      "0.35747039318084717 tensor(0.9062)\n",
      "0.25108328461647034 tensor(0.9375)\n",
      "0.5470806360244751 tensor(0.9219)\n",
      "0.35740530490875244 tensor(0.9219)\n",
      "0.3943920135498047 tensor(0.9062)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6227921843528748 tensor(0.8750)\n",
      "0.6430667638778687 tensor(0.8594)\n",
      "0.3726019263267517 tensor(0.9062)\n",
      "0.35584786534309387 tensor(0.9375)\n",
      "0.25103527307510376 tensor(0.9688)\n",
      "0.24773955345153809 tensor(0.9219)\n",
      "0.14220617711544037 tensor(0.9688)\n",
      "0.27566584944725037 tensor(0.9219)\n",
      "0.2589729428291321 tensor(0.9375)\n",
      "0.08486108481884003 tensor(0.9688)\n",
      "0.18121382594108582 tensor(0.9531)\n",
      "0.4890833795070648 tensor(0.8906)\n",
      "0.24478769302368164 tensor(0.9219)\n",
      "0.23462143540382385 tensor(0.9375)\n",
      "0.07113039493560791 tensor(1.)\n",
      "0.07848846912384033 tensor(0.9844)\n",
      "0.2308158129453659 tensor(0.9062)\n",
      "0.32970213890075684 tensor(0.9375)\n",
      "0.08422369509935379 tensor(0.9844)\n",
      "0.2909955084323883 tensor(0.9219)\n",
      "0.3838285207748413 tensor(0.8906)\n",
      "0.5017582774162292 tensor(0.9375)\n",
      "0.2560933828353882 tensor(0.9375)\n",
      "0.059464164078235626 tensor(1.)\n",
      "0.23425611853599548 tensor(0.9219)\n",
      "0.4368111789226532 tensor(0.8906)\n",
      "0.5858156085014343 tensor(0.8125)\n",
      "0.40578770637512207 tensor(0.8906)\n",
      "0.1950586885213852 tensor(0.9688)\n",
      "0.1787775605916977 tensor(0.9375)\n",
      "0.20540985465049744 tensor(0.9375)\n",
      "0.19732946157455444 tensor(0.9219)\n",
      "0.06728047877550125 tensor(0.9688)\n",
      "0.2774049937725067 tensor(0.9219)\n",
      "0.4189450740814209 tensor(0.8750)\n",
      "0.3111303746700287 tensor(0.9375)\n",
      "0.2799973487854004 tensor(0.8906)\n",
      "0.10438963770866394 tensor(0.9531)\n",
      "0.14361372590065002 tensor(0.9688)\n",
      "0.20895877480506897 tensor(0.9531)\n",
      "0.35493004322052 tensor(0.8906)\n",
      "0.17775285243988037 tensor(0.9219)\n",
      "0.25098004937171936 tensor(0.9219)\n",
      "0.18427599966526031 tensor(0.9375)\n",
      "0.4739566147327423 tensor(0.8438)\n",
      "0.13342835009098053 tensor(0.9531)\n",
      "0.3307485282421112 tensor(0.9219)\n",
      "0.25359052419662476 tensor(0.9062)\n",
      "0.43573707342147827 tensor(0.9219)\n",
      "0.284048855304718 tensor(0.9219)\n",
      "0.17648310959339142 tensor(0.9531)\n",
      "0.14836271107196808 tensor(0.9531)\n",
      "0.23185816407203674 tensor(0.9375)\n",
      "0.38565123081207275 tensor(0.8594)\n",
      "0.17356500029563904 tensor(0.9375)\n",
      "0.18507562577724457 tensor(0.9219)\n",
      "0.256267249584198 tensor(0.9531)\n",
      "0.3624948263168335 tensor(0.9062)\n",
      "0.34286069869995117 tensor(0.8906)\n",
      "0.3528674244880676 tensor(0.8750)\n",
      "0.24234849214553833 tensor(0.9062)\n",
      "0.418560266494751 tensor(0.9688)\n",
      "0.24004730582237244 tensor(0.9062)\n",
      "0.3274964690208435 tensor(0.8906)\n",
      "0.2593346834182739 tensor(0.9531)\n",
      "0.08490868657827377 tensor(0.9844)\n",
      "0.29891395568847656 tensor(0.9219)\n",
      "0.2709716260433197 tensor(0.9219)\n",
      "0.1710529923439026 tensor(0.9219)\n",
      "0.4580720365047455 tensor(0.8750)\n",
      "0.08074872940778732 tensor(0.9688)\n",
      "0.08527738600969315 tensor(0.9688)\n",
      "0.26314616203308105 tensor(0.9375)\n",
      "0.32281383872032166 tensor(0.9375)\n",
      "0.19209158420562744 tensor(0.9219)\n",
      "0.23236456513404846 tensor(0.9062)\n",
      "0.21137088537216187 tensor(0.9375)\n",
      "0.4056079387664795 tensor(0.8594)\n",
      "0.1458551436662674 tensor(0.9531)\n",
      "0.19714896380901337 tensor(0.9375)\n",
      "0.16387443244457245 tensor(0.9688)\n",
      "0.21702072024345398 tensor(0.9219)\n",
      "0.1490025520324707 tensor(0.9688)\n",
      "0.13276788592338562 tensor(0.9531)\n",
      "0.2757720649242401 tensor(0.9531)\n",
      "0.2674809992313385 tensor(0.9688)\n",
      "0.571556031703949 tensor(0.9219)\n",
      "0.24593332409858704 tensor(0.8750)\n",
      "0.39257779717445374 tensor(0.8750)\n",
      "0.3175576329231262 tensor(0.9219)\n",
      "0.5355848670005798 tensor(0.8750)\n",
      "0.594416081905365 tensor(0.8125)\n",
      "0.35432207584381104 tensor(0.8594)\n",
      "0.3168574571609497 tensor(0.9219)\n",
      "0.512464165687561 tensor(0.9375)\n",
      "0.09610293060541153 tensor(0.9844)\n",
      "0.151609867811203 tensor(0.9375)\n",
      "0.19700300693511963 tensor(0.9375)\n",
      "0.7329431772232056 tensor(0.8438)\n",
      "0.4413587152957916 tensor(0.9062)\n",
      "0.13101275265216827 tensor(0.9375)\n",
      "0.23125672340393066 tensor(0.9375)\n",
      "0.22242560982704163 tensor(0.9375)\n",
      "0.30602914094924927 tensor(0.9375)\n",
      "0.2036469280719757 tensor(0.9219)\n",
      "0.3235022723674774 tensor(0.8750)\n",
      "0.32579049468040466 tensor(0.9219)\n",
      "0.1700408011674881 tensor(0.9375)\n",
      "0.29199734330177307 tensor(0.8750)\n",
      "0.23010244965553284 tensor(0.9219)\n",
      "0.12714841961860657 tensor(0.9375)\n",
      "0.13599412143230438 tensor(0.9688)\n",
      "0.12847839295864105 tensor(0.9688)\n",
      "0.24602538347244263 tensor(0.9219)\n",
      "0.22588562965393066 tensor(0.9375)\n",
      "0.18446926772594452 tensor(0.9375)\n",
      "0.0673459842801094 tensor(0.9844)\n",
      "0.7136673331260681 tensor(0.8750)\n",
      "0.15369319915771484 tensor(0.9531)\n",
      "0.3420877754688263 tensor(0.9531)\n",
      "0.32134419679641724 tensor(0.9375)\n",
      "0.7477991580963135 tensor(0.8438)\n",
      "0.21432960033416748 tensor(0.9375)\n",
      "0.17802339792251587 tensor(0.9219)\n",
      "0.14692422747612 tensor(0.9531)\n",
      "0.09035635739564896 tensor(0.9844)\n",
      "0.27965787053108215 tensor(0.8906)\n",
      "0.11063225567340851 tensor(0.9688)\n",
      "0.2533981204032898 tensor(0.9375)\n",
      "0.34246665239334106 tensor(0.8906)\n",
      "0.34719741344451904 tensor(0.9062)\n",
      "0.24069884419441223 tensor(0.9531)\n",
      "0.2320004105567932 tensor(0.9062)\n",
      "0.27156922221183777 tensor(0.9375)\n",
      "0.14943939447402954 tensor(0.9062)\n",
      "0.20216406881809235 tensor(0.9375)\n",
      "0.2107076495885849 tensor(0.9219)\n",
      "0.23692265152931213 tensor(0.9375)\n",
      "0.36260104179382324 tensor(0.8750)\n",
      "0.1825484335422516 tensor(0.9219)\n",
      "0.4393596351146698 tensor(0.8906)\n",
      "0.35430842638015747 tensor(0.8906)\n",
      "0.35540783405303955 tensor(0.8906)\n",
      "0.27606266736984253 tensor(0.8906)\n",
      "0.3477611541748047 tensor(0.8750)\n",
      "0.35755982995033264 tensor(0.9219)\n",
      "0.31134313344955444 tensor(0.8906)\n",
      "0.19140209257602692 tensor(0.9375)\n",
      "0.15278977155685425 tensor(0.9688)\n",
      "0.21131816506385803 tensor(0.9375)\n",
      "0.31070202589035034 tensor(0.8594)\n",
      "0.2280939817428589 tensor(0.9219)\n",
      "0.36294257640838623 tensor(0.8594)\n",
      "0.28547316789627075 tensor(0.9062)\n",
      "0.31466665863990784 tensor(0.9062)\n",
      "0.25356021523475647 tensor(0.8750)\n",
      "0.3683541715145111 tensor(0.9062)\n",
      "0.21158142387866974 tensor(0.9531)\n",
      "0.33300283551216125 tensor(0.8906)\n",
      "0.3488866686820984 tensor(0.9375)\n",
      "0.49256420135498047 tensor(0.8750)\n",
      "0.5584306120872498 tensor(0.9062)\n",
      "0.29368358850479126 tensor(0.8906)\n",
      "0.6616378426551819 tensor(0.8594)\n",
      "0.45386582612991333 tensor(0.8438)\n",
      "0.3576899766921997 tensor(0.8750)\n",
      "0.13450279831886292 tensor(0.9688)\n",
      "0.27593421936035156 tensor(0.9219)\n",
      "0.3839106857776642 tensor(0.8750)\n",
      "0.5888509750366211 tensor(0.8438)\n",
      "0.3233523368835449 tensor(0.8906)\n",
      "0.2593677043914795 tensor(0.9219)\n",
      "0.23806513845920563 tensor(0.9375)\n",
      "0.3058210015296936 tensor(0.8750)\n",
      "0.33412599563598633 tensor(0.8906)\n",
      "0.2630671262741089 tensor(0.9062)\n",
      "0.4147672653198242 tensor(0.8750)\n",
      "0.26626455783843994 tensor(0.9219)\n",
      "0.4091968536376953 tensor(0.9219)\n",
      "0.409483402967453 tensor(0.8750)\n",
      "0.4108594059944153 tensor(0.8750)\n",
      "0.3237255811691284 tensor(0.9375)\n",
      "0.17904916405677795 tensor(0.9531)\n",
      "0.0789402574300766 tensor(0.9688)\n",
      "0.47625017166137695 tensor(0.8750)\n",
      "0.22940963506698608 tensor(0.9375)\n",
      "0.47286590933799744 tensor(0.9062)\n",
      "0.2247225046157837 tensor(0.9688)\n",
      "0.33802202343940735 tensor(0.9219)\n",
      "0.16058237850666046 tensor(0.9219)\n",
      "0.24586500227451324 tensor(0.9375)\n",
      "0.16857033967971802 tensor(0.9375)\n",
      "0.16375581920146942 tensor(0.9531)\n",
      "0.2405657172203064 tensor(0.9531)\n",
      "0.44529861211776733 tensor(0.9219)\n",
      "0.31680577993392944 tensor(0.9219)\n",
      "0.29100877046585083 tensor(0.8750)\n",
      "0.24917960166931152 tensor(0.9219)\n",
      "0.22657662630081177 tensor(0.9062)\n",
      "0.13894587755203247 tensor(0.9531)\n",
      "0.1457889974117279 tensor(0.9531)\n",
      "0.32656556367874146 tensor(0.9375)\n",
      "0.09051074832677841 tensor(0.9688)\n",
      "0.07801501452922821 tensor(0.9688)\n",
      "0.13009828329086304 tensor(0.9375)\n",
      "0.1732310950756073 tensor(0.9688)\n",
      "0.24684879183769226 tensor(0.9219)\n",
      "0.14844444394111633 tensor(0.9688)\n",
      "0.15819312632083893 tensor(0.9062)\n",
      "0.1511034369468689 tensor(0.9688)\n",
      "0.14396324753761292 tensor(0.9531)\n",
      "0.3223094940185547 tensor(0.9062)\n",
      "0.37141209840774536 tensor(0.8906)\n",
      "0.45496252179145813 tensor(0.8594)\n",
      "0.23769178986549377 tensor(0.8906)\n",
      "0.3957400619983673 tensor(0.9062)\n",
      "0.31992673873901367 tensor(0.9219)\n",
      "0.3409700393676758 tensor(0.8906)\n",
      "0.4330180883407593 tensor(0.8750)\n",
      "0.2553509473800659 tensor(0.8906)\n",
      "0.13690699636936188 tensor(0.9531)\n",
      "0.3071252107620239 tensor(0.9375)\n",
      "0.15380749106407166 tensor(0.9531)\n",
      "0.23873120546340942 tensor(0.9375)\n",
      "0.2990341782569885 tensor(0.9219)\n",
      "0.1485283374786377 tensor(0.9375)\n",
      "0.17427164316177368 tensor(0.9531)\n",
      "0.2474658191204071 tensor(0.9375)\n",
      "0.42518091201782227 tensor(0.8594)\n",
      "0.1477271318435669 tensor(0.9688)\n",
      "0.2809869050979614 tensor(0.9219)\n",
      "0.19103537499904633 tensor(0.9531)\n",
      "0.14846131205558777 tensor(0.9688)\n",
      "0.10725948214530945 tensor(0.9531)\n",
      "0.20314878225326538 tensor(0.8906)\n",
      "0.25958386063575745 tensor(0.9219)\n",
      "0.2403138279914856 tensor(0.9219)\n",
      "0.3330802917480469 tensor(0.9219)\n",
      "0.2588364779949188 tensor(0.9375)\n",
      "0.23224841058254242 tensor(0.9375)\n",
      "0.2538306713104248 tensor(0.9375)\n",
      "0.08271881937980652 tensor(0.9688)\n",
      "0.2635652422904968 tensor(0.9219)\n",
      "0.38220566511154175 tensor(0.9062)\n",
      "0.1614261418581009 tensor(0.9375)\n",
      "0.25514617562294006 tensor(0.9219)\n",
      "0.11960706859827042 tensor(0.9844)\n",
      "0.16873009502887726 tensor(0.9688)\n",
      "0.29731160402297974 tensor(0.9219)\n",
      "0.26506850123405457 tensor(0.9219)\n",
      "0.2389117032289505 tensor(0.9219)\n",
      "0.13705554604530334 tensor(0.9688)\n",
      "0.2495182305574417 tensor(0.9219)\n",
      "0.42350223660469055 tensor(0.8750)\n",
      "0.4399127960205078 tensor(0.8594)\n",
      "0.18503199517726898 tensor(0.9219)\n",
      "0.5132642388343811 tensor(0.8438)\n",
      "0.6671421527862549 tensor(0.8750)\n",
      "0.9270058274269104 tensor(0.7969)\n",
      "0.5352532863616943 tensor(0.8438)\n",
      "0.41842180490493774 tensor(0.8438)\n",
      "0.16680285334587097 tensor(0.9688)\n",
      "0.26458653807640076 tensor(0.9219)\n",
      "0.48020032048225403 tensor(0.8750)\n",
      "0.2696492373943329 tensor(0.9219)\n",
      "0.32360929250717163 tensor(0.9531)\n",
      "0.2560604512691498 tensor(0.9375)\n",
      "0.403231143951416 tensor(0.9219)\n",
      "0.24576202034950256 tensor(0.9219)\n",
      "0.15211501717567444 tensor(0.9531)\n",
      "0.12218426913022995 tensor(0.9688)\n",
      "0.3345915675163269 tensor(0.8906)\n",
      "0.3110945224761963 tensor(0.8906)\n",
      "0.3169156014919281 tensor(0.8750)\n",
      "0.1943301260471344 tensor(0.9219)\n",
      "0.3155195713043213 tensor(0.8906)\n",
      "0.33946168422698975 tensor(0.9219)\n",
      "0.42721250653266907 tensor(0.8750)\n",
      "0.3365360498428345 tensor(0.8281)\n",
      "0.26373422145843506 tensor(0.9219)\n",
      "0.12039006501436234 tensor(0.9531)\n",
      "0.12097706645727158 tensor(0.9688)\n",
      "0.1693730652332306 tensor(0.9531)\n",
      "0.1784660518169403 tensor(0.9688)\n",
      "0.15005901455879211 tensor(0.9844)\n",
      "0.2579439580440521 tensor(0.9062)\n",
      "0.19845904409885406 tensor(0.9375)\n",
      "0.2465490698814392 tensor(0.8750)\n",
      "0.7883102893829346 tensor(0.8125)\n",
      "0.4036170244216919 tensor(0.8750)\n",
      "0.4540572166442871 tensor(0.8750)\n",
      "0.2455253303050995 tensor(0.9375)\n",
      "0.17035838961601257 tensor(0.9375)\n",
      "0.28390711545944214 tensor(0.9219)\n",
      "0.23762895166873932 tensor(0.9688)\n",
      "0.28669631481170654 tensor(0.8750)\n",
      "0.19292335212230682 tensor(0.9531)\n",
      "0.2051817774772644 tensor(0.9531)\n",
      "0.42636215686798096 tensor(0.9062)\n",
      "0.17952997982501984 tensor(0.9219)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3293892741203308 tensor(0.8906)\n",
      "0.23576147854328156 tensor(0.9062)\n",
      "0.14227880537509918 tensor(0.9688)\n",
      "0.26062148809432983 tensor(0.9062)\n",
      "0.2831616699695587 tensor(0.9219)\n",
      "0.16247409582138062 tensor(0.9688)\n",
      "0.3400288224220276 tensor(0.8906)\n",
      "0.2824164628982544 tensor(0.9062)\n",
      "0.23890364170074463 tensor(0.9062)\n",
      "0.2371377944946289 tensor(0.9375)\n",
      "0.35533809661865234 tensor(0.9375)\n",
      "0.1547004133462906 tensor(0.9375)\n",
      "0.17498676478862762 tensor(0.9531)\n",
      "0.18756011128425598 tensor(0.9531)\n",
      "0.4759255647659302 tensor(0.8594)\n",
      "0.24561145901679993 tensor(0.9375)\n",
      "0.2552059292793274 tensor(0.9219)\n",
      "0.2267967015504837 tensor(0.9219)\n",
      "0.31477808952331543 tensor(0.9062)\n",
      "0.4832991361618042 tensor(0.8594)\n",
      "0.2853761911392212 tensor(0.8594)\n",
      "0.5951343774795532 tensor(0.8750)\n",
      "0.34996169805526733 tensor(0.9062)\n",
      "0.3562711179256439 tensor(0.8906)\n",
      "0.170293927192688 tensor(0.9219)\n",
      "0.17220252752304077 tensor(0.9375)\n",
      "0.2380416989326477 tensor(0.9531)\n",
      "0.2005981206893921 tensor(0.9219)\n",
      "0.17693860828876495 tensor(0.9375)\n",
      "0.29714235663414 tensor(0.9062)\n",
      "0.4713192582130432 tensor(0.9062)\n",
      "0.20755493640899658 tensor(0.9375)\n",
      "0.3057163953781128 tensor(0.9531)\n",
      "0.28321415185928345 tensor(0.9062)\n",
      "0.13973820209503174 tensor(0.9531)\n",
      "0.5960374474525452 tensor(0.8594)\n",
      "0.28341051936149597 tensor(0.9375)\n",
      "0.6596959829330444 tensor(0.8906)\n",
      "0.6097968816757202 tensor(0.8750)\n",
      "0.13964912295341492 tensor(0.9688)\n",
      "0.21222668886184692 tensor(0.9219)\n",
      "0.3216805160045624 tensor(0.8906)\n",
      "0.3384084403514862 tensor(0.9062)\n",
      "0.4009242653846741 tensor(0.8906)\n",
      "0.3835703730583191 tensor(0.8594)\n",
      "0.35272565484046936 tensor(0.8750)\n",
      "0.24274936318397522 tensor(0.9062)\n",
      "0.26293787360191345 tensor(0.8594)\n",
      "0.2880710959434509 tensor(0.8906)\n",
      "0.42768537998199463 tensor(0.8906)\n",
      "0.06176146864891052 tensor(1.)\n",
      "0.21905812621116638 tensor(0.9219)\n",
      "0.11535025388002396 tensor(0.9688)\n",
      "0.14058206975460052 tensor(0.9688)\n",
      "0.33097612857818604 tensor(0.9219)\n",
      "0.0827697366476059 tensor(0.9844)\n",
      "0.3394189476966858 tensor(0.9062)\n",
      "0.09374471008777618 tensor(0.9688)\n",
      "0.2248530089855194 tensor(0.9375)\n",
      "0.34937071800231934 tensor(0.9062)\n",
      "0.3663853108882904 tensor(0.9219)\n",
      "0.25107982754707336 tensor(0.9219)\n",
      "0.16341903805732727 tensor(0.9375)\n",
      "0.36557134985923767 tensor(0.8906)\n",
      "0.14426063001155853 tensor(0.9844)\n",
      "0.2993081212043762 tensor(0.9062)\n",
      "0.36959946155548096 tensor(0.8750)\n",
      "0.33684390783309937 tensor(0.9219)\n",
      "0.4533826410770416 tensor(0.9062)\n",
      "0.2075580507516861 tensor(0.9531)\n",
      "0.15838655829429626 tensor(0.9375)\n",
      "0.1608700156211853 tensor(0.9531)\n",
      "0.17082001268863678 tensor(0.9688)\n",
      "0.1934637874364853 tensor(0.9375)\n",
      "0.34737181663513184 tensor(0.9062)\n",
      "0.4986688494682312 tensor(0.8281)\n",
      "0.2962583303451538 tensor(0.9219)\n",
      "0.458387553691864 tensor(0.8906)\n",
      "0.3952796459197998 tensor(0.9062)\n",
      "0.32831692695617676 tensor(0.9062)\n",
      "0.21657586097717285 tensor(0.9375)\n",
      "0.15660113096237183 tensor(0.9219)\n",
      "0.06798303127288818 tensor(1.)\n",
      "0.2960931062698364 tensor(0.9219)\n",
      "0.5494512915611267 tensor(0.8125)\n",
      "0.2729206383228302 tensor(0.9375)\n",
      "0.47290170192718506 tensor(0.8906)\n",
      "0.20487849414348602 tensor(0.9219)\n",
      "0.15762582421302795 tensor(0.9531)\n",
      "0.502180814743042 tensor(0.9219)\n",
      "0.3138284683227539 tensor(0.8906)\n",
      "0.32860952615737915 tensor(0.9062)\n",
      "0.27610743045806885 tensor(0.8906)\n",
      "0.29934072494506836 tensor(0.8906)\n",
      "0.42497774958610535 tensor(0.8750)\n",
      "0.22428293526172638 tensor(0.9375)\n",
      "0.509946346282959 tensor(0.8906)\n",
      "0.6014074683189392 tensor(0.8750)\n",
      "0.3387007713317871 tensor(0.8750)\n",
      "0.44747626781463623 tensor(0.8750)\n",
      "0.10819604992866516 tensor(0.9688)\n",
      "0.12527209520339966 tensor(0.9688)\n",
      "0.22239342331886292 tensor(0.9062)\n",
      "0.09665165841579437 tensor(0.9844)\n",
      "0.43877267837524414 tensor(0.9062)\n",
      "0.07637432217597961 tensor(0.9844)\n",
      "0.21296250820159912 tensor(0.9219)\n",
      "0.22495010495185852 tensor(0.9219)\n",
      "0.26416927576065063 tensor(0.9219)\n",
      "0.17248742282390594 tensor(0.9531)\n",
      "0.20061182975769043 tensor(0.9375)\n",
      "0.26163825392723083 tensor(0.9219)\n",
      "0.41103652119636536 tensor(0.8281)\n",
      "0.2557682693004608 tensor(0.9531)\n",
      "0.23544205725193024 tensor(0.9219)\n",
      "0.3962501883506775 tensor(0.8750)\n",
      "0.12516295909881592 tensor(0.9844)\n",
      "0.6295282244682312 tensor(0.8438)\n",
      "0.16965337097644806 tensor(0.9375)\n",
      "0.35843178629875183 tensor(0.9219)\n",
      "0.2582176923751831 tensor(0.9375)\n",
      "0.16196157038211823 tensor(0.9531)\n",
      "0.34993183612823486 tensor(0.8906)\n",
      "0.43512025475502014 tensor(0.8906)\n",
      "0.1598517894744873 tensor(0.9844)\n",
      "0.24835842847824097 tensor(0.9531)\n",
      "0.08471599966287613 tensor(0.9844)\n",
      "0.09048597514629364 tensor(0.9688)\n",
      "0.15442843735218048 tensor(0.9531)\n",
      "0.5652883052825928 tensor(0.8125)\n",
      "0.16848579049110413 tensor(0.9531)\n",
      "0.1502891182899475 tensor(0.9688)\n",
      "0.23932784795761108 tensor(0.9375)\n",
      "0.16489887237548828 tensor(0.9375)\n",
      "0.28372201323509216 tensor(0.9219)\n",
      "0.15098649263381958 tensor(0.9531)\n",
      "0.1472218930721283 tensor(0.9531)\n",
      "0.20404298603534698 tensor(0.8906)\n",
      "0.5309222340583801 tensor(0.8750)\n",
      "0.6584572792053223 tensor(0.8281)\n",
      "0.34441056847572327 tensor(0.8906)\n",
      "0.1759008765220642 tensor(0.9219)\n",
      "0.3576053977012634 tensor(0.8750)\n",
      "0.23218737542629242 tensor(0.9219)\n",
      "0.19289696216583252 tensor(0.9688)\n",
      "0.09660062193870544 tensor(0.9688)\n",
      "0.23437298834323883 tensor(0.9219)\n",
      "0.6553723812103271 tensor(0.8125)\n",
      "0.5466353297233582 tensor(0.8281)\n",
      "0.5249130725860596 tensor(0.8438)\n",
      "0.232284277677536 tensor(0.9062)\n",
      "0.19116240739822388 tensor(0.9844)\n",
      "0.29196205735206604 tensor(0.9219)\n",
      "0.6044328808784485 tensor(0.8750)\n",
      "0.18184037506580353 tensor(0.9531)\n",
      "0.20295646786689758 tensor(0.9375)\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        # set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        print(loss.item(), accuracy(pred, yb))\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * lr\n",
    "            b -= b.grad * lr\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4005, grad_fn=<NegBackward>) tensor(0.8594)\n"
     ]
    }
   ],
   "source": [
    "r = 1000\n",
    "print(loss_func(model(x_train[r:r+bs]), y_train[r:r+bs]), accuracy(model(x_train[r:r+bs]), y_train[r:r+bs]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们要重构代码，功能不需要变化，仅仅是要使用torch.nn来让代码更简洁更灵活。接下来的每一步我们都要让代码变得更短更容易理解更灵活。\n",
    "\n",
    "第一件也是最简单的步骤是把我们手写的激活函数和损失函数替换为torch.nn.functional(通常取别名为F)。这个模块包含了torch.nn中的所有函数。包括许多损失函数和激活函数，你也能在这里找到用于构建神经网络的函数，包括池化函数。（它也包含一些卷积、全连接函数等操作，但我们将会看到，在其他库中有更好的选择）\n",
    "\n",
    "如果你使用负对数似然函数损失和softmax激活函数，那么pytorch提供了F.cross_entropy。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来验证一下，这个函数与我们之前手写的有什么不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0605, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用nn.Moduel and nn.Parameter，目的是得到一个更清晰更简单的训练过程。我们继承nn.Module。在这个例子中，我们想创建一个类来保存w和b，和forward函数。nn.Module有许多属性和方法可以派上用场。（比如.parameters()， .zero_grad()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.b = nn.Parameter(torch.zeros(10))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然我们已经使用一个对象来替换函数，我们首先要创建一个类的实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以使用跟之前一样的方法来计算loss。看起来nn.Module是一个方法，但实际上，它调用了类内部的forward函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5355, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这之前，我们必须在训练时按照参数名字更新权重，手动地把梯度清零，像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w -= w * lr\n",
    "    b -= b.grad * lr\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以利用model.parameters()和model.zero_grad()去让这些步骤更简单而且更不容易忘记一些参数。尤其是我们有一个很复杂的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个run不了，还没有梯度\n",
    "# with torch.no_grad():\n",
    "#     for p in model.parameters(): \n",
    "#         p -= p.grad * lr\n",
    "#     model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把这些函数包装到一个函数里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0812, grad_fn=<NllLossBackward>)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))\n",
    "print(accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们继续重构代码。我们不再手动定义w和b，手动计算xb @ self.w + self.b，而是使用nn.Linear作为一个layer。nn中还有许多定义好的layer使用它们可以简化我们的代码，而且通常会使代码跑的更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化一个model并且用同样的方式计算loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3289, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们仍然能够适合fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0805, grad_fn=<NllLossBackward>)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "print(loss_func(model(xb), yb))\n",
    "print(accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim里包含许多优化算法。我们能使用step函数代替手动更新参数的过程。\n",
    "\n",
    "这让我们可以把以前的优化部分代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in model.parameters(): \n",
    "        p -= p.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "替换为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt.step()\n",
    "# opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optim.zero_grad()将梯度置为零，我们需要在下一个batch重新计算梯度之前调用这个方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把能用得上的代码包装一下，避免我们重复书写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3354597091674805 0.125\n",
      "0.08091235905885696 1.0\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb).item(), accuracy(model(xb), yb).item())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb).item(), accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch有一个抽象类——Dataset。Dataset可以是任何东西，只要他有__len__函数和__getitem__函数。\n",
    "\n",
    "PyTorch的TensorDataset是一个包装了tensor的Dataset。通过定义一个length和一个索引的方式，这也就给我们一个在第一维迭代、索引、切片的方式。这让我们在一行访问独立和非独立的变量变成可能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train 和 y_train都能使用TensorDataset来绑定在一起，这让迭代和切片更容易。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前，我们不得不分别迭代获取x和y的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xb = x_train[start_i:end_i]\n",
    "#yb = y_train[start_i:end_i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们能把这两步放在一起："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = train_ds[i*bs : i*bs+bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08135267347097397 1.0\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb).item(), accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor using DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch的``DataLoader``负责管理batch。你可以从任意一个DataSet创建一个DataLoader。DataLoader让遍历变得更简单。这样我们就不必每次都用``train_ds\\[i*bs : i*bs+bs\\]``这种方式，DataLoader会自动地为我们提供mini-batch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前我们的循环长这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range((n-1)//bs + 1):\n",
    "#     xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "#     pred = model(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们的循环变得更加简洁，（xb，yb）被dataloader自动地装载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xb, yb in train_d1:\n",
    "#     pred = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08171205967664719 1.0\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (xb, yb) in enumerate(train_dl):\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(xb), yb).item(), accuracy(model(xb), yb).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为PyTorch的nn.Module,nn.Parameter,Dataset和Dataloader，我们的训练过程才可以这么简洁这么易于理解。现在让我们尝试添加一些必要的feature来让我们的模型在实际中更有效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一部分，我们在努力把训练部分的代码变得合理变得简洁。事实上，我们还需要一个验证集，用来证明我们训练的模型是否过拟合。\n",
    "\n",
    "把训练数据打乱是很重要的，这能减少batch之间的相关性并且避免过拟合。另一方面，验证集是否打乱对于验证集的loss没有影响。既然打乱需要额外的时间消耗，那么打乱验证集是没有意义的。\n",
    "\n",
    "我们将会在验证集上使用二倍于训练集的batchsize。这是因为，验证集上的计算不需要反向传播，因此也就不需要担心因存储梯度而带来的内存消耗。我们可以充分利用这一点，使用更大的batchsize来加速运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size= bs * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要在每个循环打印验证集的loss。\n",
    "注意model.train()和model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.29914346525940716 0.9139636075949367\n",
      "1 0.27666164349906053 0.9236550632911392\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "        valid_acc = sum(accuracy(model(xb), yb) for xb, yb in valid_dl)\n",
    "    \n",
    "    print(epoch, valid_loss.item() / len(valid_dl), valid_acc.item() / len(valid_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create fit() and get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们做一点自己的重构。既然我们要为训练集和验证集计算两次loss，那么不如把他们封装在一起。\n",
    "\n",
    "在训练时，我们需要使用优化器优化，但验证时不需要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit运行必要的操作去训练我们的模型，并且计算每个epoch训练集和验证集的loss。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_data返回训练集和测试集的dataloader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们获取数据和训练模型的代码只需要这三行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2947019842028618\n",
      "1 0.2878762198805809\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switch to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们要在神经网络中加一些卷积层。因为在之前的网络中我们没有对数据格式做任何要求，因此我们能够不加任何改变把它们应用在卷积神经网络中。\n",
    "\n",
    "我们直接使用Pytorch的Conv2d类作为卷积层。我们定义一个带有三个卷积层的CNN。每个卷积后面都跟着一个ReLU。最后我们用一个平均池化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)#14\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)#7\n",
    "        self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)#4\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.view(-1, 1, 28, 28)\n",
    "        xb = F.relu(self.conv1(xb))\n",
    "        xb = F.relu(self.conv2(xb))\n",
    "        xb = F.relu(self.conv3(xb))\n",
    "        xb = F.avg_pool2d(xb, 4)\n",
    "        xb = xb.view(-1, xb.size(1))\n",
    "        return xb\n",
    "\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动量是一个SGD的参数，它引入了以前的梯度更新信息，能使训练变得更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3334541584253311\n",
      "1 0.24303401448726655\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_CNN()\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn还有一个好用的类，我们可以用来简化代码——Sequential。一个Sequential对象运行每个被包含在里面的Module。这是一个更简单地书写神经网络地方法。\n",
    "\n",
    "为了充分利用这一优势，我们需要能够从一个给定的函数构建我们自己的layer。Lambda将会帮我们创建一个layer，这将在之后用Sequential定义网络时起作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上小节的CNN可以使用Sequential这样定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3226627793312073\n",
      "1 0.2517242193937302\n",
      "0.2517232298851013 0.9296000003814697\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    Lambda(preprocess),\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(4),\n",
    "    Lambda(lambda x : x.view(-1, x.size(1))),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
    "print(loss_func(model(x_valid), y_valid).item(), accuracy(model(x_valid), y_valid).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的CNN已经相当简洁了，但是它仅仅能用于MNIST数据集，因为：\n",
    "- 它假设输入是28x28\n",
    "- 它假设最终的卷积输出是4x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们抛弃这两个假设，我们的模型可以处理任意二维的单通道图像。首先我们移除Lambda layer，把数据预处理放进一个生成器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们用nn.AdaptiveAvgPool2d替换nn.AvgPool2d。它能允许我们定义想要的输出，而不是只能根据输入去计算。因此我们的模型可以处理任何尺寸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.17400281445980073\n",
      "1 0.1647987748682499\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有支持CUDA的GPU，那么你就可以使用硬件加速计算。首先检查有没有CUDA："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后创建一个device对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更新preprocess函数，把tensor移动到GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28).to(dev), y.to(dev)\n",
    "\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，把模型移动到GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(dev)\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感受一下速度吧："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14535682306289674\n",
      "1 0.13263009257316588\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经完成了一个通用的数据处理和模型训练过程，这些经历能帮助你训练其他类型的model。如果想弄清楚这个简单的模型是怎样工作的，我们可以看看mnist_sample这个notebook。\n",
    "\n",
    "当然，那还有许多事情我们可以加进来，比如数据增广，超参数调参，监控训练，迁移学习等。\n",
    "\n",
    "我们在文章的一开始就承诺我们会通过例子解释torch.nn,torch.optim,Dataset和Dataloader。现在我们来总结一下：\n",
    "\n",
    "- **torch.nn**\n",
    "    + ``Module``：一个可调用对象，用起来像一个函数，但它是一个类能存储属性（例如各种参数）。它能管理它的参数，比如更新梯度，清零梯度，遍历参数等等。\n",
    "    + ``Parameter``：一个tensor的包装器，可以保存梯度信息，但需要requires_grad=True。\n",
    "    + ``functional``：这个模块包括各种激活函数，损失函数等。还包括无状态的layer函数。比如卷积、全连接。\n",
    "- ``torch.optim``：包含各种优化器。\n",
    "- ``Dataset``：一个带有__len__和__getitem__的抽象接口。\n",
    "- ``DataLoader``：获取一个Dataset，创建一个可迭代的对象。每次返回一个batch的数据。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
